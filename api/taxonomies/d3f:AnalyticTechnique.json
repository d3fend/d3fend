{"@context":{"rdfs":"http://www.w3.org/2000/01/rdf-schema#","owl":"http://www.w3.org/2002/07/owl#","d3f":"http://d3fend.mitre.org/ontologies/d3fend.owl#","skos":"http://www.w3.org/2004/02/skos/core#"},"@graph":[{"@id":"d3f:ANN-basedClustering","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-ABC"],"d3f:definition":["Combines the principles of Artificial Neural Networks (ANN) and clustering methods."],"d3f:kb-article":["## References\nWikipedia. (n.d.). Artificial neural network. [Link](https://en.wikipedia.org/wiki/Artificial_neural_network)"],"rdfs:hasSubClass":[{"@id":"d3f:AdaptiveResonanceTheoryClustering"},{"@id":"d3f:DNN-basedClustering"},{"@id":"d3f:Self-organizingMap"}],"rdfs:label":["ANN-based Clustering"],"rdfs:subClassOf":[{"@id":"d3f:ClusterAnalysis"}]},{"@id":"d3f:ARIMAModel","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-AM"],"d3f:definition":["An autoregressive integrated moving average (ARIMA) model is a generalization of an autoregressive moving average (ARMA) model."],"d3f:kb-article":["## References\nWikipedia. (n.d.). Autoregressive integrated moving average. [Link](https://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average)"],"d3f:synonym":["Autoregressive Integrated Moving Average Model"],"rdfs:label":["ARIMA Model"],"rdfs:subClassOf":[{"@id":"d3f:TimeSeriesAnalysis"}]},{"@id":"d3f:ARMA_Model","@type":["owl:Class","owl:NamedIndividual","d3f:TimeSeriesAnalysis"],"d3f:d3fend-id":["D3-ARMA"],"d3f:definition":["Autoregressive-moving-average (ARMA) models provide a parsimonious description of a (weakly) stationary stochastic process in terms of two polynomials, one for the autoregression (AR) and the second for the moving average (MA)."],"d3f:kb-article":["## References\nWikipedia. (n.d.). Autoregressive-moving-average model. [Link](https://en.wikipedia.org/wiki/Autoregressive%E2%80%93moving-average_model)"],"d3f:synonym":["Autoregressive moving average model"],"rdfs:label":["ARMA Model"],"rdfs:subClassOf":[{"@id":"d3f:TimeSeriesAnalysis"}]},{"@id":"d3f:ActiveLearning","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-AL"],"d3f:definition":["Active learning aims to improve learning efficiency by allowing the learning algorithm to select which data to learn from."],"d3f:kb-article":["## How it works\nTraditional supervised learning often requires a large number of labeled instances, which can be costly or time-consuming to obtain. Active learning addresses this labeling bottleneck by asking an oracle (e.g., a human annotator) to label selected unlabeled instances. The goal is to achieve high accuracy with minimal labeling effort.\n\n## Considerations\nActive learning is particularly useful in scenarios where data is abundant but labeled instances are scarce or expensive. Examples include speech recognition, information extraction, and document classification.\n\n## References\n- Wikipedia article on Active Learning (machine learning) [Link](https://en.wikipedia.org/wiki/Active_learning_(machine_learning))\n- Settles, B. (2009). Active Learning Literature Survey. [Link](https://burrsettles.com/pub/settles.activelearning.pdf)"],"rdfs:hasSubClass":[{"@id":"d3f:Density-weightedMethod"},{"@id":"d3f:ExpectedErrorReduction"},{"@id":"d3f:ExpectedModelChange"},{"@id":"d3f:QueryByCommittee"},{"@id":"d3f:UncertaintySampling"},{"@id":"d3f:VarianceReduction"}],"rdfs:label":["Active Learning"],"rdfs:subClassOf":[{"@id":"d3f:MachineLearning"}]},{"@id":"d3f:Actor-Critic","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-AC"],"d3f:definition":["Actor-Critic is a Temporal Difference(TD) version of Policy gradient. It has two networks: Actor and Critic. The actor decided which action should be taken and critic inform the actor how good was the action and how it should adjust. The learning of the actor is based on policy gradient approach. In comparison, critics evaluate the action produced by the actor by computing the value function."],"d3f:kb-article":["## References\nThe Actor-Critic Reinforcement Learning Algorithm. Medium. [Link](https://medium.com/intro-to-artificial-intelligence/the-actor-critic-reinforcement-learning-algorithm-c8095a655c14)."],"rdfs:label":["Actor-Critic"],"rdfs:subClassOf":[{"@id":"d3f:TemporalDifferenceLearning"},{"@id":"d3f:PolicyGradient"}]},{"@id":"d3f:AdaptiveResonanceTheoryClustering","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-ARTC"],"d3f:definition":["Adaptive Resonance Theory (ART) Clustering is a  neural network algorithm used for clustering data and is open to new learning(i.e. adaptive) without discarding the previous or the old information(i.e. resonance)."],"d3f:kb-article":["## References\nGeeksforGeeks. (n.d.). Adaptive Resonance Theory (ART). [Link](https://www.geeksforgeeks.org/adaptive-resonance-theory-art/)"],"rdfs:label":["Adaptive Resonance Theory Clustering"],"rdfs:subClassOf":[{"@id":"d3f:ANN-basedClustering"}]},{"@id":"d3f:AgglomerativeClustering","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-AC"],"d3f:definition":["Agglomerative Clustering is a type of hierarchical clustering method where data points are grouped together based on similarity. Initially, each data point is treated as an individual cluster, and then in successive iterations, the closest clusters are merged until only one large cluster remains or until a specified stopping criterion is met."],"d3f:kb-article":["## How it works\n\nAgglomerative clustering starts with each data point as its own cluster. The algorithm then iterates, identifying the two clusters that are closest to each other based on a defined distance metric (e.g., Euclidean, Manhattan). These two clusters are then merged into a single cluster. This process continues iteratively, merging the closest pairs of clusters in each step until all data points are merged into a single cluster or until other stopping criteria are achieved. A dendrogram, which is a tree-like diagram, can be used to represent the sequence of merges, providing a visual representation of the hierarchical structure of data.\n\n## Considerations\n\n- **Choice of Distance Metric**: The outcome can vary significantly depending on the chosen distance metric (e.g., Euclidean, Manhattan).\n\n- **Scalability**: Agglomerative clustering can be computationally intensive for large datasets.\n\n- **Sensitivity**: The method can be sensitive to outliers, which might affect the quality of the clusters formed.\n\n## Key Test Considerations\n\n- **Unsupervised Learning**:\n\n  - **Number of Clusters**: Determine an optimal number of clusters using the dendrogram and techniques like the elbow method.\n\n- **Cluster Analysis**:\n\n    - **Silhouette Score**: Evaluates how similar an object is to its own cluster compared to other clusters. A higher silhouette score indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters.\n\n    - **Dunn Index**: Measures the ratio between the smallest distance between observations not in the same cluster to the largest intra-cluster distance.\n\n- **Hierarchical Clustering**:\n\n    - **Cophenetic Correlation Coefficient**: Measures the correlation between the distances of points in feature space and their distances on the dendrogram. Helps assess the fidelity of the dendrogram in preserving pairwise distances between samples.\n\n- **Agglomerative Clustering**:\n\n    - **Linkage Criteria**: Test different linkage criteria (e.g., single, complete, average) to determine which produces the most cohesive clusters for the data at hand.\n\n  ## Platforms, Tools, or Libraries\n\n- **scikit-learn**:\n\n    - A versatile machine learning library in Python.\n\n    - The `AgglomerativeClustering` class in scikit-learn provides this functionality.\n\n- **SciPy**:\n\n    - A Python library used for scientific and technical computing.\n\n    - The `scipy.cluster.hierarchy` module provides functions for hierarchical and\n    agglomerative clustering, including the `linkage` and `dendrogram` functions.\n\n- **R**:\n\n    - The `hclust` function in the stats package provides agglomerative clustering.\n\n    - The `agnes` function in the `cluster` package offers a more extensive implementation.\n\n- **MATLAB**:\n\n    - Offers the `linkage` function for hierarchical agglomerative clustering and `dendrogram` for visualization.\n\n- **Weka**:\n\n    - A collection of machine learning algorithms for data mining tasks.\n\n    - The `HierarchicalClusterer` class provides an implementation of agglomerative clustering.\n\n## References\n\n1. Jain, A. K., & Dubes, R. C. (1988). *Algorithms for clustering data*. Prentice-Hall, Inc.\n\n2. Murtagh, F., & Legendre, P. (2014). Ward’s hierarchical agglomerative clustering method: which algorithms implement Ward’s criterion?. *Journal of Classification*, 31(3), 274-295. [Link](https://link.springer.com/article/10.1007/s00357-014-9161-z).\n\n3. Scikit-learn. (30 Jun 2023). Scikit-learn Documentation: Agglomerative Clustering.\n[Link](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html)."],"rdfs:label":["Agglomerative Clustering"],"rdfs:subClassOf":[{"@id":"d3f:HierarchicalClustering"}]},{"@id":"d3f:AlethicLogic","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-AL"],"d3f:definition":["Alethic logic is a modal logic that addresses the modalities of necessity and possibility."],"d3f:kb-article":["## References\n1. Alethic logic. (2023, June 4). In _Wikipedia_. [Link](https://en.wikipedia.org/wiki/Modal_logic#Alethic_logic)"],"rdfs:label":["Alethic Logic"],"rdfs:subClassOf":[{"@id":"d3f:ModalLogic"}]},{"@id":"d3f:AnalyticTechnique","@type":"owl:Class","d3f:definition":["A process in which a computer examines information using mathematical methods in order to find useful patterns."],"rdfs:hasSubClass":[{"@id":"d3f:MachineLearning"},{"@id":"d3f:SymbolicLogic"},{"@id":"d3f:StatisticalMethod"}],"rdfs:isDefinedBy":["https://dictionary.cambridge.org/us/dictionary/english/analytics"],"rdfs:label":["Analytic Technique"],"rdfs:subClassOf":[{"@id":"d3f:D3FENDThing"}]},{"@id":"d3f:AnswerSetProgramming","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-ASP"],"d3f:definition":["Answer set programming is a form of declarative programming based on the stable model (answer set) semantics of logic programming."],"d3f:kb-article":["## How it works\nAnswer set programming (ASP) is oriented towards difficult (primarily NP-hard) search problems. The computational process employed in the design of many answer set solvers is an enhancement of the DPLL algorithm and, in principle, it always terminates (unlike Prolog query evaluation, which may lead to an infinite loop).\n\nIn a more general sense, ASP includes all applications of answer sets to knowledge representation and the use of Prolog-style query evaluation for solving problems arising in these applications.\n\n## References\n1. Answer set programming. (2023, April 27). In _Wikipedia_. [Link](https://en.wikipedia.org/wiki/Answer_set_programming)"],"d3f:synonym":["ASP"],"rdfs:label":["Answer Set Programming"],"rdfs:subClassOf":[{"@id":"d3f:LogicProgramming"}]},{"@id":"d3f:ApproximateStringMatching","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-ASM"],"d3f:definition":["Approximate string matching is a form of string matching that allows errrors."],"d3f:kb-article":["## References\n1. Navarro, G. (2001). A guided tour to approximate string matching. _ACM Computing Surveys_, 33(1), 31-88. [Link](https://doi.org/10.1145/375360.375365)"],"rdfs:hasSubClass":[{"@id":"d3f:LevenshteinMatching"}],"rdfs:label":["Approximate String Matching"],"rdfs:subClassOf":[{"@id":"d3f:PartialMatching"}]},{"@id":"d3f:ArtificialNeuralNetClassification","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-annotation":["Classification ANNs seek to classify an observation as belonging to some discrete class as a function of the inputs. The input features (independent variables) can be categorical or numeric types, however, we require a categorical feature as the dependent variable."],"d3f:d3fend-id":["D3A-ANNC"],"d3f:kb-article":["## References\nANN Classification. [Link](http://uc-r.github.io/ann_classification)."],"rdfs:hasSubClass":[{"@id":"d3f:MultilayerPerceptronClassification"},{"@id":"d3f:DeepNeuralNetClassification"}],"rdfs:label":["Artificial Neural Network Classification"],"rdfs:subClassOf":[{"@id":"d3f:Classification"}]},{"@id":"d3f:AssociationRuleLearning","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-ARL"],"d3f:definition":["Association rule learning is a rule-based machine learning method for discovering interesting relations between variables in large databases."],"d3f:kb-article":["## References\nAssociation rule learning. (n.d.). Wikipedia. [Link](https://en.wikipedia.org/wiki/Association_rule_learning)"],"rdfs:label":["Association Rule Learning"],"rdfs:subClassOf":[{"@id":"d3f:UnsupervisedLearning"}]},{"@id":"d3f:AsymmetricFeature-basedTransferLearning","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-AFTL"],"d3f:definition":["Homogeneous (where the metrics are the same for both source and target) asymmetric transformation mapping transforms the source feature space to align with that of the target or the target to that of the source. This, in effect, bridges the feature space gap and reduces the problem into a homogeneous transfer problem when further distribution differences need to be corrected."],"d3f:kb-article":["## References\nDay, O., & Khoshgoftaar, T.M. (2017). A survey on heterogeneous transfer learning. Journal of Big Data, 4(1), 29. [Link](https://doi.org/10.1186/s40537-017-0089-0)."],"rdfs:label":["Asymmetric Feature-based Transfer Learning"],"rdfs:subClassOf":[{"@id":"d3f:HomogenousTransferLearning"}]},{"@id":"d3f:Autoencoding","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-AUT"],"d3f:definition":["Autoencoders are specific type of deep learning architecture used for learning representation of data, typically for the purpose of dimensionality reduction. This is achieved by designing deep learning architecture that aims that copying input layer at its output layer."],"d3f:kb-article":["## References\nSOCR. (n.d.). ABIDE Autoencoder. [Link](https://socr.umich.edu/HTML5/ABIDE_Autoencoder/#:~:text=In%20simple%20words%2C%20autoencoders%20are,layer%20at%20its%20output%20layer.)"],"rdfs:label":["Autoencoding"],"rdfs:subClassOf":[{"@id":"d3f:DimensionReduction"}]},{"@id":"d3f:AutoregressiveModel","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-AM"],"d3f:definition":["An autoregressive (AR) model is a representation of a type of random process; as such, it is used to describe certain time-varying processes in nature, economics, behavior, etc."],"d3f:kb-article":["## References\nWikipedia. (n.d.). Autoregressive model. [Link](https://en.wikipedia.org/wiki/Autoregressive_model)"],"d3f:synonym":["AR Model"],"rdfs:label":["Autoregressive Model"],"rdfs:subClassOf":[{"@id":"d3f:TimeSeriesAnalysis"}]},{"@id":"d3f:AverageAbsoluteDeviation","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-AAD"],"d3f:definition":["The average absolute deviation (AAD) of a data set is the average of the absolute deviations from a central point."],"d3f:kb-article":["## References\nWikipedia. (n.d.). Average absolute deviation. [Link](https://en.wikipedia.org/wiki/Average_absolute_deviation)"],"rdfs:hasSubClass":[{"@id":"d3f:MeanAbsoluteDeviation"},{"@id":"d3f:MedianAbsoluteDeviation"}],"rdfs:label":["Average Absolute Deviation"],"rdfs:subClassOf":[{"@id":"d3f:Variability"}]},{"@id":"d3f:BERT","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-BER"],"d3f:definition":["Bidirectional Encoder Representations from Transformers (BERT) is based on a deep learning model in which every output element is connected to every input element, and the weightings between them are dynamically calculated based upon their connection."],"d3f:kb-article":["## References\nBERT (language model). (n.d.). In TechTarget. [Link](https://www.techtarget.com/searchenterpriseai/definition/BERT-language-model)\nBERT (language model). (n.d.). In Wikipedia. [Link](https://en.wikipedia.org/wiki/BERT_(language_model))"],"d3f:synonym":["Bidirectional Encoder Representations from Transformers"],"rdfs:label":["BERT"],"rdfs:subClassOf":[{"@id":"d3f:Transformer-basedLearning"}]},{"@id":"d3f:BayesOptimalClassifier","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-BOC"],"d3f:definition":["A probabilistic model that makes the most probable prediction for a new example."],"d3f:kb-article":["## References\nBayes Optimal Classifier. Machine Learning Mastery.  [Link](https://machinelearningmastery.com/bayes-optimal-classifier/).\nEnsemble learning. Wikipedia.  [Link](https://en.wikipedia.org/wiki/Ensemble_learning)."],"rdfs:label":["Bayes Optimal Classifier"],"rdfs:subClassOf":[{"@id":"d3f:EnsembleLearning"}]},{"@id":"d3f:BayesianEstimation","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-BE"],"d3f:definition":["A Bayes estimator or a Bayes action is an estimator or decision rule that minimizes the posterior expected value of a loss function (i.e., the posterior expected loss)."],"d3f:kb-article":["## References\nWikipedia. (n.d.). Bayes estimator. [Link](https://en.wikipedia.org/wiki/Bayes_estimator)"],"rdfs:label":["Bayesian Estimation"],"rdfs:subClassOf":[{"@id":"d3f:BayesianMethod"}]},{"@id":"d3f:BayesianHypothesisTesting","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-BHT"],"d3f:definition":["Bayesian hypothesis testing can be framed as a special case of model comparison where a model refers to a likelihood function and a prior distribution."],"d3f:kb-article":["## How it works\nGiven two competing hypotheses and some relevant data, Bayesian hypothesis testing begins by specifying separate prior distributions to quantitatively describe each hypothesis. The combination of the likelihood function for the observed data with each of the prior distributions yields hypothesis-specific models. For each of the hypothesis-specific models, averaging (ie, integrating) the likelihood with respect to the prior distribution across the entire parameter space yields the probability of the data under the model and, therefore, the corresponding hypothesis. This quantity is more commonly referred to as the marginal likelihood and represents the average fit of the model to the data. The ratio of the marginal likelihoods for both hypothesis-specific models is known as the Bayes factor.\n\n## References\nBaig, S. A., PhD. (2020). Bayesian Inference: An Introduction to Hypothesis Testing Using Bayes Factors. Nicotine & Tobacco Research, 22(7), 1244-1246. [Link](https://academic.oup.com/ntr/article/22/7/1244/5613971)"],"rdfs:label":["Bayesian Hypothesis Testing"],"rdfs:subClassOf":[{"@id":"d3f:BayesianMethod"}]},{"@id":"d3f:BayesianLinearRegression","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-BLR"],"d3f:definition":["Bayesian linear regression is a type of conditional modeling in which the mean of one variable is described by a linear combination of other variables, with the goal of obtaining the posterior probability of the regression coefficients."],"d3f:kb-article":["## References\nWikipedia. (n.d.). Bayesian linear regression. [Link](https://en.wikipedia.org/wiki/Bayesian_linear_regression)"],"rdfs:label":["Bayesian Linear Regression"],"rdfs:subClassOf":[{"@id":"d3f:RegressionAnalysis"}]},{"@id":"d3f:BayesianLinearRegressionLearning","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-BLRL"],"d3f:definition":["A supervised learning method that builds a Bayesian linear regression model using training data."],"d3f:kb-article":["## References\nWikipedia. (n.d.). Bayesian linear regression. [Link](https://en.wikipedia.org/wiki/Bayesian_linear_regression)"],"rdfs:label":["Bayesian Linear Regression Learning"],"rdfs:seeAlso":["http://d3fend.mitre.org/ontologies/d3fend.owl#BayesianLinearRegression"],"rdfs:subClassOf":[{"@id":"d3f:RegressionAnalysisLearning"}]},{"@id":"d3f:BayesianMethod","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-BM"],"d3f:definition":["Bayesian analysis is a statistical procedure which endeavors to estimate parameters of an underlying distribution based on the observed distribution."],"d3f:kb-article":["## References\nWolfram MathWorld. (n.d.). Bayesian Analysis. [Link](https://mathworld.wolfram.com/BayesianAnalysis.html)"],"rdfs:hasSubClass":[{"@id":"d3f:BayesianEstimation"},{"@id":"d3f:BayesianHypothesisTesting"}],"rdfs:label":["Bayesian Method"],"rdfs:subClassOf":[{"@id":"d3f:StatisticalMethod"}]},{"@id":"d3f:BayesianModelAveraging","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-BMA"],"d3f:definition":["A parameter estimate (or a prediction of new observations) obtained by averaging the estimates (or predictions) of the different models under consideration, each weighted by its model probability."],"d3f:kb-article":["## References\nEnsemble learning. Wikipedia.  [Link](https://en.wikipedia.org/wiki/Ensemble_learning).\n\nBayesian model average: A parameter estimation approach to model agnostic ensemble learning. (2019). Journal of Machine Learning for Modeling and Computing, 1(2), 61-70.  [Link](https://journals.sagepub.com/doi/full/10.1177/2515245919898657#:~:text=Bayesian%20model%20average%3A%20A%20parameter,weighted%20by%20its%20model%20probability)."],"rdfs:label":["Bayesian Model Averaging"],"rdfs:subClassOf":[{"@id":"d3f:EnsembleLearning"}]},{"@id":"d3f:BayesianModelCombination","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-BMC"],"d3f:definition":["Bayesian model combination (BMC) is an algorithmic correction to Bayesian model averaging (BMA). Instead of sampling each model in the ensemble individually, it samples from the space of possible ensembles (with model weights drawn randomly from a Dirichlet distribution having uniform parameters)"],"d3f:kb-article":["## References\nEnsemble learning. Wikipedia.  [Link](https://en.wikipedia.org/wiki/Ensemble_learning).\n\nShultz, K. M., & Peterson, L. E. (2011). Model-averaged confidence intervals for ensemble learning. In *International Joint Conference on Neural Networks* (pp. 2677-2684).  [Link](https://axon.cs.byu.edu/papers/Kristine.ijcnn2011.pdf)."],"rdfs:label":["Bayesian Model Combination"],"rdfs:subClassOf":[{"@id":"d3f:EnsembleLearning"}]},{"@id":"d3f:BooleanExpressionMatching","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-BEM"],"d3f:definition":["Boolean expression matching produces a Boolean truth value for a given boolean expression and assignment of values to variables in the expression."],"d3f:kb-article":["## How it works\nA Boolean expression is an expression used in programming languages that produces a Boolean value when evaluated. A Boolean value is either true or false. A Boolean expression may be composed of a combination of the Boolean constants true or false, Boolean-typed variables, Boolean-valued operators, and Boolean-valued functions.\n\nBoolean expressions correspond to propositional formulas in logic and are a special case of Boolean circuits.\n\n## References\n1. Boolean expression. (2022, April 25). In _Wikipedia_. [Link](https://en.wikipedia.org/wiki/Boolean_expression)\n2. Boolean algebra. (2022, May 19). In _Wikipedia_.\n[Link](https://en.wikipedia.org/wiki/Boolean_expression)"],"rdfs:label":["Boolean Expression Matching"],"rdfs:subClassOf":[{"@id":"d3f:LogicalRules"}]},{"@id":"d3f:Boosting","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-BOO"],"d3f:definition":["Boosting is a sequential process where each subsequent model attempts to correct the errors of the previous model"],"d3f:kb-article":["## How it works\nBoosting consists of using sequentially weak learners where each iteration’s training focuses on previously misclassified instances in order to improve on the previous iteration. This process is continued iteratively until the final prediction is made by aggregating the previous predictions.\n\n## Considerations\nBoosting can be computationally expensive, prone to overfitting, and slower to train compared to other ensemble methods.\n\nThere are three main types of Boosting algorithms\n - Adaptive Boosting\nAdaptive Boosting (sometimes called AdaBoost) works by adding equal importance to each piece of a dataset and running it through the base learning algorithms. Every algorithm that errors, the boosting algorithm assigns a higher importance to. This continues until an acceptable level of confidence is reached.\n - Gradient Boosting\nGradient Boosting starts by training multiple models simultaneously to gather a strong estimate of strength to build new base learning algorithms.\n - XGBoosting\nXGBoosting is a scalable tree boosting model. Using decision trees, weight is assigned to each variable and put into a decision tree. Outputs that are classified by the algorithm as wrong or weak are put into a second decision tree and the results form a stronger model.\n\n## References\nSciencedirect. (n.d.). Semi-supervised learning: An overview. [Link](https://www.sciencedirect.com/science/article/pii/S1319157823000228)"],"rdfs:label":["Boosting"],"rdfs:subClassOf":[{"@id":"d3f:EnsembleLearning"}]},{"@id":"d3f:BootstrapAggregating","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-BA"],"d3f:definition":["Bootstrap aggregating, also called bagging (from bootstrap aggregating), is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also reduces variance and helps to avoid overfitting. Although it is usually applied to decision tree methods, it can be used with any type of method. Bagging is a special case of the model averaging approach."],"d3f:kb-article":["## References\nBootstrap aggregating. Wikipedia.  [Link](https://en.wikipedia.org/wiki/Bootstrap_aggregating)."],"rdfs:hasSubClass":[{"@id":"d3f:RandomForest"}],"rdfs:label":["Bootstrap Aggregating"],"rdfs:subClassOf":[{"@id":"d3f:ResamplingEnsemble"}]},{"@id":"d3f:BucketOfModels","@type":["owl:Class","owl:NamedIndividual"],"d3f:d3fend-id":["D3A-BOM"],"d3f:definition":["A \"bucket of models\" is an ensemble technique in which a model selection algorithm is used to choose the best model for each problem. When tested with only one problem, a bucket of models can produce no better results than the best model in the set, but when evaluated across many problems, it will typically produce much better results, on average, than any model in the set."],"d3f:kb-article":["## References\nEnsemble learning. Wikipedia.  [Link](https://en.wikipedia.org/wiki/Ensemble_learning)."],"rdfs:label":["Bucket of Models"],"rdfs:subClassOf":[{"@id":"d3f:EnsembleLearning"}]},{"@id":"d3f:C4.5","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-C4."],"d3f:definition":["C4.5 is an algorithm that is strongly based off ID3. It creates decision trees the same way as ID3. C4.5 improves on several aspects of ID3, including handling discreet variables, handling training data with missing values, and has the ability to automatically prune the decision trees it creates."],"d3f:kb-article":["## References\nC4.5 algorithm. Wikipedia. [Link](https://en.wikipedia.org/wiki/C4.5_algorithm)."],"rdfs:label":["C4.5"],"rdfs:subClassOf":[{"@id":"d3f:DecisionTree"}]},{"@id":"d3f:C5.0","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-C5."],"d3f:definition":["C5.0 is the next version of C4.5, which in turn is the upgrade from ID3. The only difference between C5.0 and C4.5 is some improvements made to C5.0."],"d3f:kb-article":["## References\nC4.5 algorithm. Wikipedia. [Link](https://en.wikipedia.org/wiki/C4.5_algorithm)."],"rdfs:label":["C5.0"],"rdfs:subClassOf":[{"@id":"d3f:DecisionTree"}]},{"@id":"d3f:CART","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-CAR"],"d3f:definition":["The CART algorithm is a type of classification algorithm that is required to build a decision tree on the basis of Gini’s impurity index."],"d3f:kb-article":["## References\nClassification and Regression Tree (CART) Algorithm. Analytics Steps. [Link](https://www.analyticssteps.com/blogs/classification-and-regression-tree-cart-algorithm)."],"rdfs:hasSubClass":[{"@id":"d3f:GradientBoostedDecisionTree"}],"rdfs:label":["CART"],"rdfs:subClassOf":[{"@id":"d3f:DecisionTree"}]},{"@id":"d3f:CanopyClustering","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-CC"],"d3f:definition":["The canopy clustering algorithm is an unsupervised pre-clustering algorithm  often used as preprocessing step for the K-means algorithm or the Hierarchical clustering algorithm. It is intended to speed up clustering operations on large data sets."],"d3f:kb-article":["## References\nWikipedia. (n.d.). Canopy clustering algorithm. [Link](https://en.wikipedia.org/wiki/Canopy_clustering_algorithm)"],"rdfs:label":["Canopy Clustering"],"rdfs:subClassOf":[{"@id":"d3f:ClusterAnalysis"}]},{"@id":"d3f:CentralTendency","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-CT"],"d3f:definition":["A measure of central tendency ) is a summary measure that attempts to describe a whole set of data with a single value that represents the middle or centre of its distribution."],"d3f:kb-article":["## References\nAustralian Bureau of Statistics. (n.d.). Measures of Central Tendency. [Link](https://www.abs.gov.au/statistics/understanding-statistics/statistical-terms-and-concepts/measures-central-tendency)\n\nWikipedia. (n.d.). Central tendency. [Link](https://en.wikipedia.org/wiki/Central_tendency)"],"rdfs:hasSubClass":[{"@id":"d3f:Mean"},{"@id":"d3f:Median"},{"@id":"d3f:Mode"},{"@id":"d3f:GeometricMean"},{"@id":"d3f:HarmonicMean"},{"@id":"d3f:TrimmedMean"},{"@id":"d3f:WeightedMean"}],"rdfs:label":["Central Tendency"],"rdfs:subClassOf":[{"@id":"d3f:DescriptiveStatistics"}]},{"@id":"d3f:Centroid-basedClustering","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-CBC"],"d3f:definition":["Centroid-based clustering organizes the data into non-hierarchical clusters, in contrast to hierarchical clustering defined below. K-means is the most widely-used centroid-based clustering algorithm. Centroid-based algorithms are efficient but sensitive to initial conditions and outliers."],"d3f:kb-article":["## References\nGoogle Developers. (n.d.). Clustering Algorithms. [Link](https://developers.google.com/machine-learning/clustering/clustering-algorithms)"],"rdfs:hasSubClass":[{"@id":"d3f:K-meansClustering"}],"rdfs:label":["Centroid-based Clustering"],"rdfs:subClassOf":[{"@id":"d3f:ClusterAnalysis"}]},{"@id":"d3f:Classification","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-CLA"],"d3f:definition":["Classification uses an algorithm to accurately assign test data into specific categories."],"d3f:kb-article":["## How it works\nClassification recognizes specific entities within the dataset and attempts to draw some conclusions on how those entities should be labeled or defined. Common classification algorithms are linear classifiers, support vector machines (SVM), decision trees, k-nearest neighbor, and random forest, which are described in more detail below.\n\n## Considerations:\n\nThere are many different types of classification algorithms for modeling classification predictive modeling problems.\n\nThere is no single theory on how to map algorithms onto problem types; instead, it is generally recommended that a practitioner use controlled experiments and discover which algorithm and algorithm configuration results in the best performance for a given classification task.\n\n## Key Test Considerations\n\n- **Machine Learning**:\n\n  - **Verify the dataset quality**: Check the data to make sure it is\n      free of errors.  Quantify the degree of missing values,\n      outliers, and noise in the data collection.  If the data quality\n      is low, it may be difficult or impossible to create models and\n      systems with the desired performance.\n\n  - **Verify development datasets are representative** of expected\n      operational environment and data collection means.  Compare\n      distributions of dataset features and labels with exploratory\n      data analysis and assess the difference in tests on training\n      data and tests on evaluation data (where the evaluation data\n      must be drawn from a representative dataset.)\n\n  - **Use software libraries**: and tools built for ML where possible, so\n      that the underlying code is verified by prior use.**\n\n  - **Diagnose model errors with domain SMEs**: Have problem domain\n    SMEs investigate model errors for conditions for which the model\n    may underperform and suggest refinements.\n\n- **Classification**:\n\n  - **Use Standard Classification Performance Measures**: Not all of\n      the following may be necessary, but should be considered for\n      both verification (developmental test) and operational test\n      stages use:\n\n    - **Accuracy**: The fraction of predictions that were corret.\n\n    - **Precision**: The proportion of positive identifications that were correct.\n\n    - **Recall**: The proportion of actual positive cases identified correctly.\n\n    - **F-Measure**: Combines the preicion and recall into a single\n        score.  It is the harmonic mean of the precision and recall.\n\n    - **Receiver Operating Characteristic (ROC) Curve**: A ROC curve\n        shows the performance of a classification model at all\n        classification thresholds.  It graphs the True Positive Rate\n        over the False Positive Rate.\n\n    - **Area Under the ROC Curve (AUC)**: This measures the\n        two-dimensional area under the ROC Curve.  AUC is\n        scale-invariant and classification-threshold invariant.\n\n    - **ROC TP vs FP points**: In addition to a specific AUC score,\n        the performance at points\n\n    - **Confusion Matrix**: A confusion matrix is a table layout that\n        allows the visualization of the performance of an\n        algorithm. Each row of the matrix represents the instances in\n        an actual class while each column represents the instances in\n        a predicted class, or vice versa. It is a special kind of\n        contingency table, with two dimensions (\"actual\" and\n        \"predicted\"), and identical sets of \"classes\" in both\n        dimensions (each combination of dimension and class is a\n        variable in the contingency table.)\n\n  - **Prediction Bias**: The difference between the average of the\n      predicted labels and the average of the labels in the data\n      set.  One should check for prediction bias when evaluating the\n      classifier's results. Causes of bias can include:\n\n    - **Noisy data set**: Errors in original data can as the\n      collection method may have an underlying bias.\n\n    - **Processing bug**: Errors in the data pipeline can\n      introduce bias.\n\n    - **Biased training sample (unbalanced samples)**: Model\n      parameters may be skewed towards majority classes.\n\n\t- **Overly strong regularization**: Model may be underfitting\n        model and too simple.\n\n\t- **Proxy variables**: Model features may be highly\n        correlated.\n\n  - **Overfitting and Underfitting**: Overfitting occurs when the the\n    model built corresponds too closely or exactly to a particular\n    set of data, and thus may fail to fit to predict additional data\n    reliably. An overfitted model is a mathematical model that\n    contains more parameters than can be justified by the data.\n    Underfitting occurs when the model built does adequately capture\n    the patterns in the data. As an example, a linear model will\n    underfit a non-linear dataset.\n\n## Platforms, Tools, or Libraries:\n\n- **Python**:\n\n  - **scikit-learn**: Is a free software machine learning library for\n      Python and includes features for classification.\n\n  - **TensorFlow**: is an end-to-end source machine learning\n    platform.\n\n  - **Keras**: is an open-source library that provides a Python API\n    designed to enable fast experimentation with deep neural networks.\n\n  - **PyTorch**: Is a machine learning framework based on the Torch\n    library.\n\n- **R**:\n\n  - **caret**: Classification And REgression Training package contains\n      functions to streamline model training for complex regression\n      and classification problems.\n\n  - **randomForest**: Implementation of classification and regression\n      based on forest of trees.\n\n## References\n\n1. Supervised Learning. IBM.\n[Link](https://www.ibm.com/topics/supervised-learning).\n\n1. Types of Classification in Machine Learning. Machine Learning Mastery.\n[Link](https://machinelearningmastery.com/types-of-classification-in-machine-learning/).\n\n1. Google. (18 July 2022). Classification: Precision and Recall.\n[Link](https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall).\n\n1. Wikipedia. (18 Aug 2023). Overfitting.\n[Link](https://en.wikipedia.org/wiki/Overfitting).\n\n1. Wikipedia. (19 Aug 2023). Confusion matrix.\n[Link](https://en.wikipedia.org/wiki/Confusion_matrix)."],"rdfs:hasSubClass":[{"@id":"d3f:NaiveBayesClassifier"},{"@id":"d3f:K-NearestNeighbors"},{"@id":"d3f:ArtificialNeuralNetClassification"},{"@id":"d3f:DecisionTree"},{"@id":"d3f:SupportVectorMachineClassification"},{"@id":"d3f:LinearClassifier"}],"rdfs:label":["Classification"],"rdfs:subClassOf":[{"@id":"d3f:SupervisedLearning"}]},{"@id":"d3f:ClusterAnalysis","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-CA"],"d3f:definition":["Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters)."],"d3f:kb-article":["## References\nCluster analysis. (n.d.). Wikipedia. [Link](https://en.wikipedia.org/wiki/Cluster_analysis)"],"rdfs:hasSubClass":[{"@id":"d3f:ANN-basedClustering"},{"@id":"d3f:CanopyClustering"},{"@id":"d3f:Centroid-basedClustering"},{"@id":"d3f:Density-basedClustering"},{"@id":"d3f:Distribution-basedClustering"},{"@id":"d3f:Graph-basedClustering"},{"@id":"d3f:High-dimensionClustering"},{"@id":"d3f:HierarchicalClustering"}],"rdfs:label":["Cluster Analysis"],"rdfs:subClassOf":[{"@id":"d3f:UnsupervisedLearning"}]},{"@id":"d3f:CoefficientOfVariation","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-COV"],"d3f:definition":["The coefficient of variation (CV), also known as relative standard deviation (RSD), is a standardized measure of dispersion of a probability distribution or frequency distribution.\n\nThe coefficient of variation (CV) is defined as the ratio of the standard deviation to the mean ."],"d3f:kb-article":["## References\nWikipedia. (n.d.). Coefficient of variation. [Link](https://en.wikipedia.org/wiki/Coefficient_of_variation)"],"d3f:synonym":["Relative Standard Deviation","RSD"],"rdfs:label":["Coefficient of Variation"],"rdfs:subClassOf":[{"@id":"d3f:Variability"}]},{"@id":"d3f:ConvolutionalNeuralNetwork","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-CNN"],"d3f:definition":["A class of artificial neural network most commonly applied to analyze visual imagery.CNNs use a mathematical operation called convolution in place of general matrix multiplication in at least one of their layers."],"d3f:kb-article":["## References\nWikipedia. (n.d.). Convolutional neural network. [Link](https://en.wikipedia.org/wiki/Convolutional_neural_network)"],"rdfs:hasSubClass":[{"@id":"d3f:ResidualNeuralNetwork"},{"@id":"d3f:Grid-CNN"}],"rdfs:label":["Convolutional Neural Network"],"rdfs:subClassOf":[{"@id":"d3f:DeepNeuralNetClassification"}]},{"@id":"d3f:Correlation","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-COR"],"d3f:definition":["Correlation is the degree to which two or more quantities are linearly associated."],"d3f:kb-article":["Wolfram MathWorld. (n.d.). Correlation. [Link](https://mathworld.wolfram.com/Correlation.html)"],"rdfs:hasSubClass":[{"@id":"d3f:PearsonsCorrelationCoefficient"},{"@id":"d3f:Point-biserialCorrelationCoefficient"},{"@id":"d3f:RankCorrelationCoefficient"},{"@id":"d3f:PhiCoefficient"},{"@id":"d3f:CramersV"}],"rdfs:label":["Correlation"],"rdfs:subClassOf":[{"@id":"d3f:DescriptiveStatistics"}]},{"@id":"d3f:CorrelationClustering","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-CC"],"d3f:definition":["Correlation clustering provides a method for clustering a set of objects into the optimum number of clusters without specifying that number in advance."],"d3f:kb-article":["## References\nWikipedia. (n.d.). Correlation clustering. [Link](https://en.wikipedia.org/wiki/Correlation_clustering)"],"rdfs:hasSubClass":[{"@id":"d3f:SubspaceClustering"}],"rdfs:label":["Correlation Clustering"],"rdfs:subClassOf":[{"@id":"d3f:High-dimensionClustering"}]},{"@id":"d3f:CramersV","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-CV"],"d3f:definition":["Cramér's V (sometimes referred to as Cramér's phi and denoted as φc) is a measure of association between two nominal variables, giving a value between 0 and +1 (inclusive) and is based on Pearson's chi-squared statistic."],"d3f:kb-article":["## References\nWikipedia. (n.d.). Cramér's V. [Link](https://en.wikipedia.org/wiki/Cram%C3%A9r%27s_V)"],"d3f:synonym":["Cramer's Phi"],"rdfs:label":["Cramer's V"],"rdfs:subClassOf":[{"@id":"d3f:Correlation"}]},{"@id":"d3f:CycleGAN","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-CYC"],"d3f:definition":["The Cycle Generative Adversarial Network (CycleGAN) is an approach to training a deep convolutional neural network for image-to-image translation tasks by mapping between input and output images using unpaired dataset."],"d3f:kb-article":["## References\nEsri. (n.d.). How CycleGAN Works. [Link](https://developers.arcgis.com/python/guide/how-cyclegan-works/)"],"rdfs:label":["CycleGAN"],"rdfs:subClassOf":[{"@id":"d3f:Image-to-ImageTranslationGAN"}]},{"@id":"d3f:DBSCAN","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-DBS"],"d3f:definition":["A density-based clustering algorithm that works on the assumption that clusters are dense regions in space separated by regions of lower density."],"d3f:kb-article":["## References\nAnalytics Vidhya. (2020, September 15). How DBSCAN Clustering Works: A Comprehensive Guide with Implementations in Python. [Link](https://www.analyticsvidhya.com/blog/2020/09/how-dbscan-clustering-works/#:~:text=DBSCAN%20is%20a%20density%2Dbased,points%20into%20a%20single%20cluster.)"],"rdfs:label":["DBSCAN"],"rdfs:subClassOf":[{"@id":"d3f:Density-basedClustering"}]},{"@id":"d3f:DNN-basedClustering","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-DBC"],"d3f:definition":["DNNs serve for clustering as mappings to better representations. The features of these representations can be drawn from different layers of the network or even from several layers."],"d3f:kb-article":["## References\nOpenReview. (n.d.). Unsupervised Clustering using Pseudo Ensemble Models. [Link](https://openreview.net/pdf?id=B1eT9VMgOX)"],"rdfs:label":["DNN-based Clustering"],"rdfs:subClassOf":[{"@id":"d3f:ANN-basedClustering"}]},{"@id":"d3f:Datalog","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-DAT"],"d3f:definition":["Datalog is a declarative logic programming language that is a syntactically a subset of Prolog."],"d3f:kb-article":["## How it works\nDatalog generally uses a bottom-up rather than top-down evaluation model. This difference yields significantly different behavior and properties from Prolog. It is often used as a query language for deductive databases. Datalog has been applied to problems in data integration, networking, program analysis, and more.\n\n## References\n1. Datalog. (2023, April 20). In _Wikipedia_. [Link](https://en.wikipedia.org/wiki/Datalog)"],"rdfs:label":["Datalog"],"rdfs:subClassOf":[{"@id":"d3f:LogicProgramming"}]},{"@id":"d3f:DecisionTree","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-DT"],"d3f:definition":["Decision tree learning is a supervised learning approach used in statistics, data mining, and machine learning. In this formalism, a classification or regression decision tree is used as a predictive model to draw conclusions about a set of observations."],"d3f:kb-article":["## How it works\n\nA decision tree starts with a root node, which does not have any incoming branches. The outgoing branches from the root node then feed into the internal nodes, also known as decision nodes. Based on the available features, both node types conduct evaluations to form homogenous subsets, which are denoted by leaf nodes, or terminal nodes. The leaf nodes represent all the possible outcomes within the dataset.\n\n## Considerations\n\nWhile the basic underlying model is that of a decision tree, the decision tree node criteria, and the method for identifying splits varies significantly depending on the learning algorithm selected (e.g., CART, ID3, C4.5, C5.0, CHAID, MARS.)  Extensions like linear and logistic trees can add additional expressiveness as well.\n\n## Key Test Considerations\n\n- **Machine Learning**:\n\n  - **Verify the dataset quality**: Check the data to make sure it is\n      free of errors.  Quantify the degree of missing values,\n      outliers, and noise in the data collection.  If the data quality\n      is low, it may be difficult or impossible to create models and\n      systems with the desired performance.\n\n  - **Verify development datasets are representative**: of expected\n      operational environment and data collection means.  Compare\n      distributions of dataset features and labels with exploratory\n      data analysis and assess the difference in tests on training\n      data and tests on evaluation data (where the evaluation data\n      must be drawn from a representative dataset.)\n\n   - **Use a variety of data sets**: where available and applicable, to\n      reflect different operating and environment conditions that are\n      likley to be be encountered.\n\n  - **Use software libraries**: and tools built for ML where possible, so\n      that the underlying code is verified by prior use.**\n\n  - **Diagnose model errors with domain SMEs**: Have problem domain\n    SMEs investigate model errors for conditions for which the model\n    may underperform and suggest refinements.\n\n- **Classification**:\n\n  - **Use Standard Classification Performance Measures**: Not all of\n      the following may be necessary, but should be considered for\n      both verification (developmental test) and operational test\n      stages use:\n\n    - **Accuracy**: The fraction of predictions that were corret.\n\n    - **Precision**: The proportion of positive identifications that were correct.\n\n    - **Recall**: The proportion of actual positive cases identified correctly.\n\n    - **F-Measure**: Combines the preicion and recall into a single\n        score.  It is the harmonic mean of the precision and recall.\n\n    - **Receiver Operating Characteristic (ROC) Curve**: A ROC curve\n        shows the performance of a classification model at all\n        classification thresholds.  It graphs the True Positive Rate\n        over the False Positive Rate.\n\n    - **Area Under the ROC Curve (AUC)**: This measures the\n        two-dimensional area under the ROC Curve.  AUC is\n        scale-invariant and classification-threshold invariant.\n\n    - **ROC TP vs FP points**: In addition to a specific AUC score,\n        the performance at points\n\n    - **Confusion Matrix**: A confusion matrix is a table layout that\n        allows the visualization of the performance of an\n        algorithm. Each row of the matrix represents the instances in\n        an actual class while each column represents the instances in\n        a predicted class, or vice versa. It is a special kind of\n        contingency table, with two dimensions (\"actual\" and\n        \"predicted\"), and identical sets of \"classes\" in both\n        dimensions (each combination of dimension and class is a\n        variable in the contingency table.)\n\n  - **Prediction Bias**: The difference between the average of the\n      predicted labels and the average of the labels in the data\n      set.  One should check for prediction bias when evaluating the\n      classifier's results. Causes of bias can include:\n\n    - **Noisy data set**: Errors in original data can as the\n      collection method may have an underlying bias.\n\n    - **Processing bug**: Errors in the data pipeline can\n      introduce bias.\n\n    - **Biased training sample (unbalanced samples)**: Model\n      parameters may be skewed towards majority classes.\n\n    - **Overly strong regularization**: Model may be underfitting\n       model and too simple.\n\n    - **Proxy variables**: Model features may be highly\n       correlated.\n\n- **Supervised Learning**:\n\n  - **Overfitting and Underfitting**: Overfitting occurs when the the\n    model built corresponds too closely or exactly to a particular\n    set of data, and thus may fail to fit to predict additional data\n    reliably. An overfitted model is a mathematical model that\n    contains more parameters than can be justified by the data.\n    Underfitting occurs when the model built does adequately capture\n    the patterns in the data. As an example, a linear model will\n    underfit a non-linear dataset.\n\n  - **Sensitivity**: Perform N-fold Cross validation to indicate how\n    much sensitivity the algorithm has to data variation and to avoid\n    overfitting operational models.\n\n- **Decision Tree Learning**:\n\n  - **Sensitive to unbalanced classes**: Examine and determine target\n      class balance; decision tree learning algorithms are especially\n      sensitive to unbalanced target classes.\n\n  - **Consider decision boundaries**: Perform exploratory data\n      analysis to determine if decision boundaries lie alongaxes of\n      features. _Decision trees are ideal when decision boundaries can\n      be found that lie along axes of features._\n\n   - **Decision tree overfitting** may require tuning algorithm hyperparameters such as tree depth, max features used, max leaf nodes, etc.\n\n   - **Pruning** may result in a more robust model in real-word applications.\n\n   - **Missing values**: Inspect the data set to determine if there\n     are missing values and select a means to address them, either by\n     choosing an algorithm that works well or a way to impute the\n     value or eliminate the missing values in the data sensors or\n     pipeline.\n\n## Platforms, Tools, or Libraries\n\n- **scikit-learn**: includes tree algorithms for ID3, C4.5, C5.0, and CART.\n\n- **Weka**: includes J48 (C4.5), SimpleCart (CART), Logistic Model Trees, Naive Bayes Trees, and more.\n\n### Validation Approach\n- Use operationally relevant data across the range of application's operating environment.\n- Incorporate some kind of continuous validation to address concept drift and the need to retrain the model and/or check data quality.\n\n## References\n1. Decision tree learning. (2023, May 30). In _Wikipedia_. [Link](https://en.wikipedia.org/wiki/Decision_tree_learning).\n2. Decision Trees. (n.d.). In _scikit-learn User Guide 1.2.2_. [Link](https://scikit-learn.org/stable/modules/tree.html).\n3. Concept drift. (2023, April 17). In _Wikipedia_. [Link](https://en.wikipedia.org/wiki/Concept_drift).\n4. 8 Concept Drift Detection Methods. (n.d.). In _Aporia Learning Center_. [Link](https://www.aporia.com/learn/data-drift/concept-drift-detection-methods/)."],"rdfs:hasSubClass":[{"@id":"d3f:C4.5"},{"@id":"d3f:C5.0"},{"@id":"d3f:CART"},{"@id":"d3f:ID3"}],"rdfs:label":["Decision Tree"],"rdfs:subClassOf":[{"@id":"d3f:Classification"}]},{"@id":"d3f:DecisionTreeRegression","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-DTR"],"d3f:definition":["Decision Trees Regression is asupervised learning method with the goal  to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features"],"d3f:kb-article":["## References\nscikit-learn. (n.d.). Decision Trees. [Link](https://scikit-learn.org/stable/modules/tree.html#tree)"],"rdfs:label":["Decision Tree Regression"],"rdfs:subClassOf":[{"@id":"d3f:RegressionAnalysisLearning"}]},{"@id":"d3f:DeepConvolutionalGAN","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-DCG"],"d3f:definition":["Deep Convolutional GAN (DCGAN) uses convolutional and convolutional-transpose layers in the generator and discriminator, respectively."],"d3f:kb-article":["## References\nAnalytics Vidhya. (2021). Deep Convolutional Generative Adversarial Network (DCGAN) for Beginners. [Link](https://www.analyticsvidhya.com/blog/2021/07/deep-convolutional-generative-adversarial-network-dcgan-for-beginners/)"],"d3f:synonym":["DCGAN"],"rdfs:label":["Deep Convolutional GAN"],"rdfs:subClassOf":[{"@id":"d3f:ImageSynthesisGAN"}]},{"@id":"d3f:DeepNeuralNetClassification","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-DNNC"],"d3f:definition":["A deep neural network (DNN) is an artificial neural network (ANN) with multiple layers between the input and output layers. There are different types of neural networks but they always consist of the same components: neurons, synapses, weights, biases, and functions. These components as a whole function similarly to a human brain, and can be trained like any other ML algorithm"],"d3f:kb-article":["## References\nDeep learning. Wikipedia. [Link](https://en.wikipedia.org/wiki/Deep_learning)."],"rdfs:hasSubClass":[{"@id":"d3f:RecurrentNeuralNetwork"},{"@id":"d3f:ConvolutionalNeuralNetwork"}],"rdfs:label":["Deep Neural Network Classification"],"rdfs:subClassOf":[{"@id":"d3f:ArtificialNeuralNetClassification"}]},{"@id":"d3f:DeepQ-learning","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-DQL"],"d3f:definition":["Uses a deep convolutional neural network, with layers of tiled convolutional filters to mimic the effects of receptive fields."],"d3f:kb-article":["## References\nQ-learning. Wikipedia.  [Link](https://en.wikipedia.org/wiki/Q-learning#Deep_Q-learning)."],"rdfs:label":["Deep Q-learning"],"rdfs:subClassOf":[{"@id":"d3f:Q-Learning"}]},{"@id":"d3f:Density-basedClustering","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-DBC"],"d3f:definition":["Density-based clustering connects areas of high example density into clusters. This allows for arbitrary-shaped distributions as long as dense areas can be connected."],"d3f:kb-article":["## References\nGoogle Developers. (n.d.). Clustering algorithms. [Link](https://developers.google.com/machine-learning/clustering/clustering-algorithms)"],"rdfs:hasSubClass":[{"@id":"d3f:DBSCAN"}],"rdfs:label":["Density-based Clustering"],"rdfs:subClassOf":[{"@id":"d3f:ClusterAnalysis"}]},{"@id":"d3f:Density-weightedMethod","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-DWM"],"d3f:definition":["An Actvie Learning technique that uses a density estimate meta-parameter to avoid sampling sparsely populated regions of the feature space and can be based parametrically or from a parameter free model."],"d3f:kb-article":["## References\nIntro to Active Learning. inovex Blog. [Link](https://www.inovex.de/de/blog/intro-to-active-learning/)."],"rdfs:label":["Density-weighted Method"],"rdfs:subClassOf":[{"@id":"d3f:ActiveLearning"}]},{"@id":"d3f:DeonticLogic","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-DL"],"d3f:definition":["Deontic logic addresses the modality of obligations and norms; i.e., the modality of morality."],"d3f:kb-article":["## References\n1. Deontic logic. (2023, June 4). In _Wikipedia_. [Link](https://en.wikipedia.org/wiki/Modal_logic#Deontic_logic)"],"rdfs:label":["Deontic Logic"],"rdfs:subClassOf":[{"@id":"d3f:ModalLogic"}]},{"@id":"d3f:DescriptionLogic","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-DL"],"d3f:definition":["A description logic (DL) is a form of logic usually more expressive than propositional logic but less expressive than first-order logic."],"d3f:kb-article":["## How it works\nThe core reasoning problems for description logics (DLs) are (usually) decidable, and efficient decision procedures have been designed and implemented for these problems. There are general, spatial, temporal, spatiotemporal, and fuzzy description logics, and each description logic features a different balance between expressive power and reasoning complexity by supporting different sets of mathematical constructors.\n\nDLs are used in artificial intelligence to describe and reason about the relevant concepts of an application domain (known as terminological knowledge). It is of particular importance in providing a logical formalism for ontologies and the Semantic Web: the Web Ontology Language (OWL) and its profiles are based on DLs.\n\n## References\n1. Description logic. (2023, April 16). In _Wikipedia_. [Link](https://en.wikipedia.org/wiki/Description_logic)"],"rdfs:hasSubClass":[{"@id":"d3f:OWL"}],"rdfs:label":["Description Logic"],"rdfs:subClassOf":[{"@id":"d3f:SymbolicAI"}]},{"@id":"d3f:DescriptiveStatistics","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-DS"],"d3f:definition":["Descriptive statistics provide simple summaries about the sample and about the observations that have been made."],"d3f:kb-article":["## References\nWikipedia. (n.d.). Descriptive statistics. [Link](https://en.wikipedia.org/wiki/Descriptive_statistics)"],"rdfs:hasSubClass":[{"@id":"d3f:CentralTendency"},{"@id":"d3f:Correlation"},{"@id":"d3f:DistributionProperties"},{"@id":"d3f:Variability"}],"rdfs:label":["Descriptive Statistics"],"rdfs:subClassOf":[{"@id":"d3f:StatisticalMethod"}]},{"@id":"d3f:DimensionReduction","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-DR"],"d3f:definition":["Dimensionality reduction is a key technique within unsupervised learning. It compresses the data by finding a smaller, different set of variables that capture what matters most in the original features, while minimizing the loss of information."],"d3f:kb-article":["## References\nO'Reilly Media. (n.d.). Chapter 7. Machine Learning and Security: Protecting Systems with Data and Algorithms. [Link](https://www.oreilly.com/library/view/machine-learning-and/9781492073048/ch07.html)"],"rdfs:hasSubClass":[{"@id":"d3f:PrincipalComponentsAnalysis"},{"@id":"d3f:SingularValueDecomposition"},{"@id":"d3f:Autoencoding"}],"rdfs:label":["Dimension Reduction"],"rdfs:subClassOf":[{"@id":"d3f:UnsupervisedLearning"}]},{"@id":"d3f:DiscriminantAnalysis","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-DA"],"d3f:definition":["Discriminant analysis attempts to establish whether a set of variables can be used to distinguish between two or more groups of cases."],"d3f:kb-article":["## References\nWikipedia. (n.d.). Multivariate statistics. [Link](https://en.wikipedia.org/wiki/Multivariate_statistics)"],"rdfs:label":["Discriminant Analysis"],"rdfs:subClassOf":[{"@id":"d3f:MultivariateAnalysis"}]},{"@id":"d3f:Distribution-basedClustering","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-DBC"],"d3f:definition":["Distribution-based clustering creates and groups data points based on their likely hood of belonging to the same probability distribution (Gaussian, Binomial, etc.) in the data."],"d3f:kb-article":["## References\nAnalytixLabs. (n.d.). Types of Clustering Algorithms. [Link](https://www.analytixlabs.co.in/blog/types-of-clustering-algorithms/#:~:text=Distribution-Based)"],"rdfs:hasSubClass":[{"@id":"d3f:Expectation-maximizationClustering"}],"rdfs:label":["Distribution-based Clustering"],"rdfs:subClassOf":[{"@id":"d3f:ClusterAnalysis"}]},{"@id":"d3f:DistributionProperties","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-DP"],"d3f:definition":["The properties derived from a probability distribution."],"d3f:kb-article":["## References\nWikipedia. (n.d.). Probability distribution. [Link](https://en.wikipedia.org/wiki/Probability_distribution)"],"rdfs:hasSubClass":[{"@id":"d3f:Kurtosis"},{"@id":"d3f:Moments"},{"@id":"d3f:Skewness"}],"rdfs:label":["Distribution Properties"],"rdfs:subClassOf":[{"@id":"d3f:DescriptiveStatistics"}]},{"@id":"d3f:DivisiveClustering","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-DC"],"d3f:definition":["A divisive clustering approach is a hierarchical, top-down approach to clustering a dataset."],"rdfs:label":["Divisive Clustering"],"rdfs:subClassOf":[{"@id":"d3f:HierarchicalClustering"}]},{"@id":"d3f:Dyna-Q","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-DQ"],"d3f:definition":["A Dyna-Q agent combines acting, learning, and planning."],"d3f:kb-article":["## References\nCompNeuro Neuromatch Academy Tutorials. [Link](https://compneuro.neuromatch.io/tutorials/W3D4_ReinforcementLearning/student/W3D4_Tutorial4.html)"],"rdfs:label":["Dyna-Q"],"rdfs:subClassOf":[{"@id":"d3f:Model-basedReinforcementLearning"}]},{"@id":"d3f:EnsembleLearning","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-EL"],"d3f:definition":["In statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone"],"d3f:kb-article":["## References\nEnsemble learning. Wikipedia.  [Link](https://en.wikipedia.org/wiki/Ensemble_learning)."],"rdfs:hasSubClass":[{"@id":"d3f:BayesianModelCombination"},{"@id":"d3f:BayesOptimalClassifier"},{"@id":"d3f:BayesianModelAveraging"},{"@id":"d3f:Boosting"},{"@id":"d3f:ResamplingEnsemble"},{"@id":"d3f:BucketOfModels"},{"@id":"d3f:Stacking"},{"@id":"d3f:Voting"}],"rdfs:label":["Ensemble Learning"],"rdfs:subClassOf":[{"@id":"d3f:MachineLearning"}]},{"@id":"d3f:EpistemicLogic","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-EL"],"d3f:definition":["Epistemic logic addresses modalities of knowledge; i.e., the certainty of sentences."],"d3f:kb-article":["## References\n1. Epistemic logic. (2023, June 4). In _Wikipedia_. [Link](https://en.wikipedia.org/wiki/Modal_logic#Epistemic_logic)"],"d3f:synonym":["Epistemic Modal Logic"],"rdfs:label":["Epistemic Logic"],"rdfs:subClassOf":[{"@id":"d3f:ModalLogic"}]},{"@id":"d3f:EquivalenceMatching","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-EM"],"d3f:definition":["Equivalence matching is matching two values, which may be bound to variables, to see if they are equivalent."],"d3f:kb-article":["## How it works\nEquality is a relationship between two quantities or, more generally two mathematical expressions, asserting that the quantities have the same value, or that the expressions represent the same mathematical object.\n\nProgramming languages can have multiple senses of equality that may include, but are not limited to:\n\n- Identity: The objects are identical; often indicated by having values indicating the same logical address.\n- Equality: The values of the expessions and properties are equivalent when evaluated; they do not need to have the same logical address.\n\n## References\n1. Equality (mathematics). (2023, May 31). In _Wikipedia_. [Link](https://en.wikipedia.org/wiki/Equality_(mathematics)]\n2. Types of Equality. (2007, March 2). In _WikiWikiWeb_. [Link](https://wiki.c2.com/?TypesOfEquality)"],"rdfs:hasSubClass":[{"@id":"d3f:ExactMatching"},{"@id":"d3f:StringEquivalenceMatching"}],"rdfs:label":["Equivalence Matching"],"rdfs:subClassOf":[{"@id":"d3f:LogicalRules"}]},{"@id":"d3f:Estimation","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-EST"],"d3f:definition":["Estimation represents ways or a process of learning and determining the population parameter based on the model fitted to the data."],"d3f:kb-article":["## References\nPennsylvania State University. (n.d.). Statistical Inference and Estimation. [Link](https://online.stat.psu.edu/stat504/lesson/statistical-inference-and-estimation)"],"rdfs:hasSubClass":[{"@id":"d3f:IntervalEstimation"},{"@id":"d3f:PointEstimation"}],"rdfs:label":["Estimation"],"rdfs:subClassOf":[{"@id":"d3f:InferentialStatistics"}]},{"@id":"d3f:ExactMatching","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-EM"],"d3f:definition":["Exact matching for numeric types is just the simple test for mathematical equivalence of the values being matched."],"d3f:kb-article":["## References\n1. Equality (mathematics). (2023, May 31). In _Wikipedia_. [Link](https://en.wikipedia.org/wiki/Equality_(mathematics)]"],"d3f:synonym":["Numeric Equivalence Matching"],"rdfs:label":["Exact Matching"],"rdfs:subClassOf":[{"@id":"d3f:EquivalenceMatching"},{"@id":"d3f:NumericPatternMatching"}]},{"@id":"d3f:Expectation-maximizationClustering","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-EMC"],"d3f:definition":["An unsupervised clustering algorithm and extends to NLP applications like Latent Dirichlet Allocation, the Baum-Welch algorithm for Hidden Markov Models, and medical imaging."],"d3f:kb-article":["## References\nTowards Data Science. (n.d.). Expectation Maximization Explained. [Link](https://towardsdatascience.com/expectation-maximization-explained-c82f5ed438e5#:~:text=Expectation%20Maximization%20(EM)%20is%20a,Markov%20Models%2C%20and%20medical%20imaging.)"],"rdfs:label":["Expectation-maximization Clustering"],"rdfs:subClassOf":[{"@id":"d3f:Distribution-basedClustering"}]},{"@id":"d3f:ExpectedErrorReduction","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-EER"],"d3f:definition":["Expected Error Reduction (EER) follows similar ideas as EMC, but again looks at the model output instead of the model itself and also takes the other data into account. In particular, a sample x is considered useful, if we can expect that knowing the label will reduce the future error on unseen samples"],"d3f:kb-article":["## References\nIntro to Active Learning. inovex Blog.  [Link](https://www.inovex.de/de/blog/intro-to-active-learning/)."],"rdfs:label":["Expected Error Reduction"],"rdfs:subClassOf":[{"@id":"d3f:ActiveLearning"}]},{"@id":"d3f:ExpectedModelChange","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-EMC"],"d3f:definition":["Supervised learning establishes a relationship between the known input and output variables to conduct a predictive analysis."],"d3f:kb-article":["nal Consiterations\n\n## References\nIntro to Active Learning. inovex Blog. [Link](https://www.inovex.de/de/blog/intro-to-active-learning/)."],"rdfs:label":["Expected Model Change"],"rdfs:subClassOf":[{"@id":"d3f:ActiveLearning"}]},{"@id":"d3f:First-orderLogic","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-FOL"],"d3f:definition":["First-order logic is a collection of formal systems used in mathematics, philosophy, linguistics, and computer science. First-order logic uses quantified variables over non-logical objects, and allows the use of sentences that contain variables."],"d3f:kb-article":["## How it works\n\nFor propositions such as \"Socrates is a man\", one can have expressions in the form \"there exists x such that x is Socrates and x is a man\", where \"there exists\" is a quantifier, while x is a variable. This distinguishes it from propositional logic, which does not use quantifiers or relations.\n\nThe term \"first-order\" distinguishes first-order logic from higher-order logic, in which there are predicates having predicates or functions as arguments, or in which quantification over predicates, functions, or both, are permitted.\n\n## Considerations\n\n- Advantages:\n-- First-order logic is more expressive than propositional logic; one can talk about objects and their properties, relations between objects.\n-- First-order logic is able to make use of variables and quantifiers (e.g., \"for all\" and \"exists\".)\n-- First-order logic supports power forms of reasoning, such as inferring the properties of an unknown object from the properties of known objects.\n\n- Disadvantages:\n-- First-order logic is more difficult to learn and use than propositional logic, due to its greater complexity.\n-- First-order logic is also less tractable than propositional logic in many cases; reasoning about quantifiers and variables adds complexity.\n-- First-order logic can be difficult to apply in practice, due to the need to find appropriate axioms and rules for each application.\n\n### Verification Approach\n\n- Automated theorem provers can assist in formal verification, performing automated reasoning over system modeled in first-order logic and explore a complete space of system behaviors\n- First-order logic may be more expressive than necessary for many types of problems and may be more difficult to verify by SMEs.\n- Theorem provers based in FOL are capable of use in software verification tasks, but an SMT solver such as Z3 might be more appropriate.\n- Defining a set of competency questions (i.e., query use cases for a first-order logic ontology) can help scope the logic required for a complete solution.\n\n### Validation Approach\n\n- Domain SMEs should be identified to review the analytics results and compare them to expected results for a given input.\n- Where possible, an outside team of SMEs should inspect the formal logic specification of a system against its stated requirements and suitability to address its domain problem sets.\n- Defining a set of competency questions and the expected results provides one means of validation.\n\n## References\n\n1.  First-order logic. (2023, May 26). In _Wikipedia_.  [Link](https://en.wikipedia.org/wiki/First-order_logic)\n2. Shapiro, S. and Kissel, T. Classical Logic. (2022). Stanford Encyclopedia of Philosophy. [Link](https://plato.stanford.edu/entries/logic-classical/)\n3. A.I. For Anyone. First-order Logic (n.d.). [Link](https://www.aiforanyone.org/glossary/first-order-logic)\n4. Smith, P. An Introduction to Formal Logic. (2020). [Link](https://doi.org/10.1017/9781108328999)\n5. Gruninger, M. and Fox, M. (1995). Methodology for the Design and Evaluation of Ontologies. [Link](https://www.researchgate.net/publication/2288533_Methodology_for_the_Design_and_Evaluation_of_Ontologies)\n6. Keet, C., Suarez-Figurosa, M., and Poveda-Villalon, M. (2014). Pitfalls in Ontologies and TIPS to Prevent Them. [Link](https://dl.acm.org/doi/10.4018/ijswis.2014040102)\n7. Bjorner, N. et al. The inner magic behind the Z3 theorem prover. (2019) [Link](https://www.microsoft.com/en-us/research/blog/the-inner-magic-behind-the-z3-theorem-prover/)"],"d3f:synonym":["First-order Predicate Calculus","FOL","Quantificational Logic"],"rdfs:label":["First-order Logic"],"rdfs:subClassOf":[{"@id":"d3f:PredicateLogic"}]},{"@id":"d3f:FuzzyLogic","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-FL"],"d3f:definition":["Fuzzy logic is a form of many-valued logic in which the truth value of variables may be any real number between 0 and 1."],"d3f:kb-article":["## How it works\nIt is employed to handle the concept of partial truth, where the truth value may range between completely true and completely false.[1] By contrast, in Boolean logic, the truth values of variables may only be the integer values 0 or 1.\n\n## References\n1. Fuzzy logic. (2023, May 28). In _Wikipedia_. [Link](https://en.wikipedia.org/wiki/Fuzzy_logic)"],"rdfs:label":["Fuzzy Logic"],"rdfs:subClassOf":[{"@id":"d3f:SymbolicAI"}]},{"@id":"d3f:GPT","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-GPT"],"d3f:definition":["Generative pre-trained transformers (GPT) are a type of large language model (LLM) and a prominent framework for generative artificial intelligence."],"d3f:kb-article":["## References\nGenerative pre-trained transformer. (n.d.). In Wikipedia. [Link](https://en.wikipedia.org/wiki/Generative_pre-trained_transformer)"],"d3f:synonym":["Generative Pre-trained Transformer"],"rdfs:label":["GPT"],"rdfs:subClassOf":[{"@id":"d3f:Transformer-basedLearning"}]},{"@id":"d3f:GatedRecurrentUnit","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-GRU"],"d3f:definition":["The GRU is like a long short-term memory (LSTM) with a forget gate, but has fewer parameters than LSTM, as it lacks an output gate. GRU's performance on certain tasks of polyphonic music modeling, speech signal modeling and natural language processing was found to be similar to that of LSTM"],"d3f:kb-article":["## References\nWikipedia. (2021, September 20). Gated Recurrent Unit. [Link](https://en.wikipedia.org/wiki/Gated_recurrent_unit)"],"rdfs:label":["Gated Recurrent Unit"],"rdfs:subClassOf":[{"@id":"d3f:RecurrentNeuralNetwork"}]},{"@id":"d3f:GenerativeAdversarialNetwork","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-GAN"],"d3f:definition":["Generative Adversarial Networks (GAN) are an approach to generative modeling using deep learning methods, such as convolutional neural networks."],"d3f:kb-article":["## References\nBrownlee, J. (2019). What Are Generative Adversarial Networks (GANs)? Machine Learning Mastery. [Link](https://machinelearningmastery.com/what-are-generative-adversarial-networks-gans/)"],"d3f:synonym":["GAN"],"rdfs:hasSubClass":[{"@id":"d3f:Image-to-ImageTranslationGAN"},{"@id":"d3f:ImageSynthesisGAN"},{"@id":"d3f:SeqGAN"}],"rdfs:label":["Generative Adversarial Network"],"rdfs:subClassOf":[{"@id":"d3f:UnsupervisedLearning"}]},{"@id":"d3f:GeometricMean","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-GM"],"d3f:definition":["The nth root of the product of the data values, where there are n of these. This measure is valid only for data that are measured absolutely on a strictly positive scale."],"d3f:kb-article":["## References\nWikipedia. (n.d.). Central tendency. [Link](https://en.wikipedia.org/wiki/Central_tendency)"],"rdfs:label":["Geometric Mean"],"rdfs:subClassOf":[{"@id":"d3f:CentralTendency"}]},{"@id":"d3f:GoodmanAndKruskalsGamma","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-GAKG"],"d3f:definition":["Goodman-Kruskal $\\\\gamma$ is a measure of rank correlation between x and y and is given by $(n_c -n_d) / (n_c + n_d)$, where $n_c$ is the number of concordant pairs of the observations and $n_d$ is the number of discordant pairs."],"d3f:kb-article":["## References\n1. Wolfram Research. (2012). GoodmanKruskalGamma. Wolfram Language function.  [Link](https://reference.wolfram.com/language/ref/GoodmanKruskalGamma.html)\n1. Goodman and Kruskal's gamma. (2022, Nov 23). In _Wikipedia_. [Link](https://en.wikipedia.org/wiki/Goodman_and_Kruskal%27s_gamma]"],"rdfs:isDefinedBy":["https://reference.wolfram.com/language/ref/GoodmanKruskalGamma.html"],"rdfs:label":["Goodman and Kruskal's Gamma"],"rdfs:subClassOf":[{"@id":"d3f:RankCorrelationCoefficient"}]},{"@id":"d3f:GradientBoostedDecisionTree","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-GBDT"],"d3f:definition":["A gradient-boosted decision tree is, as in other bagging and boosting methods, a method where the relatively 'weak' machine learning model (a decision tree) is used in an ensemble to form a 'strong' machine learning model."],"d3f:kb-article":["## Reference\n\n1. Google. (28 Sep 2023). Gradient Boosted Decision Trees.\n[Link](https://developers.google.com/machine-learning/decision-forests/intro-to-gbdt)."],"rdfs:label":["Gradient-Boosted Decision Tree"],"rdfs:subClassOf":[{"@id":"d3f:CART"}]},{"@id":"d3f:Graph-basedClustering","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-GBC"],"d3f:definition":["Graph-based Clustering is a form of clustering where data is represented with graphs to identify clusters.  We include Connection-based Clustering in this class."],"d3f:kb-article":["## References\n1. Jagota, A. (13 Dec 2020). Density-based and Graph-based Clustering. towardsdatascience.com. [Link](https://towardsdatascience.com/density-based-and-graph-based-clustering-a1f0d45ff5fb)\n\n1. Connectivity-Based Clustering. Sarang, P. (2023) in Thinking Data Science. The Springer Series in Applied Machine Learning. Springer, Cham. [Link](https://doi.org/10.1007/978-3-031-02363-7_10)."],"d3f:synonym":["Connection-based Clustering"],"rdfs:hasSubClass":[{"@id":"d3f:SpectralClustering"},{"@id":"d3f:K-CenterClustering"}],"rdfs:label":["Graph-based Clustering"],"rdfs:subClassOf":[{"@id":"d3f:ClusterAnalysis"}]},{"@id":"d3f:Graph-basedSemi-supervisedLearning","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-GBSSL"],"d3f:definition":["Graph-based Semi-Supervised Learning (GSSL) methods aim to classify unlabeled data by learning the graph structure and labeled data jointly."],"d3f:kb-article":["## References\nYang, S., Pan, L., & Cheng, J. (2021). Graph-based Semi-Supervised Learning Methods for Imbalanced Data Classification. [Link](https://www.sciencedirect.com/science/article/pii/S0031320321002132?viewFullText=true)."],"rdfs:label":["Graph-based Semi-supervised Learning"],"rdfs:subClassOf":[{"@id":"d3f:Semi-supervisedTransductiveLearning"}]},{"@id":"d3f:Grid-CNN","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-GC"],"d3f:definition":["A class of neural networks that specializes in processing data that has a grid-like topology, such as an image."],"d3f:kb-article":["## References\nTalukdar, P. (2020, June 10). Convolutional Neural Networks Explained. Towards Data Science. [Link](https://towardsdatascience.com/convolutional-neural-networks-explained-9cc5188c4939)"],"rdfs:label":["Grid-CNN"],"rdfs:subClassOf":[{"@id":"d3f:ConvolutionalNeuralNetwork"}]},{"@id":"d3f:Grid-basedClustering","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-GBC"],"d3f:definition":["Divides the entire data space into a finite number of cells reducing the complexity of the data and focuses on the cells rather than the data."],"d3f:kb-article":["## References\nTechVidvan. (n.d.). Clustering in Machine Learning Tutorial. [Link](https://techvidvan.com/tutorials/clustering-in-machine-learning/)"],"rdfs:label":["Grid-based Clustering"],"rdfs:subClassOf":[{"@id":"d3f:High-dimensionClustering"}]},{"@id":"d3f:HarmonicMean","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-HM"],"d3f:definition":["The reciprocal of the arithmetic mean of the reciprocals of the data values. This measure too is valid only for data that are measured absolutely on a strictly positive scale."],"d3f:kb-article":["## References\nWikipedia. (n.d.). Central tendency. [Link](https://en.wikipedia.org/wiki/Central_tendency)"],"rdfs:label":["Harmonic Mean"],"rdfs:subClassOf":[{"@id":"d3f:CentralTendency"}]},{"@id":"d3f:HeterogeneousAsymmetricFeature-basedTransferLearning","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-HAFBTL"],"d3f:definition":["Asymmetric transformation mapping  transforms the source feature space to align with that of the target or the target to that of the source. This, in effect, bridges the feature space gap and reduces the problem into a homogeneous transfer problem when further distribution differences need to be corrected."],"d3f:kb-article":["## References\nWang, Q., Mao, K. Z., Wang, B., & Guan, J. (2017). Big data clustering by hybrid optimization algorithm. Journal of Big Data, 4(1), 25. [Link](https://journalofbigdata.springeropen.com/articles/10.1186/s40537-017-0089-0)."],"rdfs:label":["Heterogeneous Asymmetric Feature-based Transfer Learning"],"rdfs:subClassOf":[{"@id":"d3f:HeterogeneousTransferLearning"}]},{"@id":"d3f:HeterogeneousFeature-basedTransferLearning","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-HFBTL"],"d3f:definition":["Symmetric transformation  takes both the source feature space Xs and target feature space Xt and learns feature transformations as to project each onto a common subspace Xc for adaptation purposes. This derived subspace becomes a domain-invariant feature subspace to associate cross-domain data, and in effect, reduces marginal distribution differences."],"d3f:kb-article":["## References\nWang, Q., Mao, K. Z., Wang, B., & Guan, J. (2017). Big data clustering by hybrid optimization algorithm. Journal of Big Data, 4(1), 25. [Link](https://journalofbigdata.springeropen.com/articles/10.1186/s40537-017-0089-0)."],"rdfs:label":["Heterogeneous Feature-based Transfer Learning"],"rdfs:subClassOf":[{"@id":"d3f:HeterogeneousTransferLearning"}]},{"@id":"d3f:HeterogeneousTransferLearning","@type":["owl:Class","owl:NamedIndividual"],"d3f:d3fend-id":["D3A-HTL"],"d3f:definition":["Heterogeneous transfer learning is characterized by the source and target domains having differing feature spaces, but may also be combined with other issues such as differing data distributions and label spaces."],"d3f:kb-article":["## References\nWang, Q., Mao, K. Z., Wang, B., & Guan, J. (2017). Big data clustering by hybrid optimization algorithm. Journal of Big Data, 4(1), 25. [Link](https://journalofbigdata.springeropen.com/articles/10.1186/s40537-017-0089-0)."],"rdfs:hasSubClass":[{"@id":"d3f:HeterogeneousAsymmetricFeature-basedTransferLearning"},{"@id":"d3f:HeterogeneousFeature-basedTransferLearning"}],"rdfs:label":["Heterogeneous Transfer Learning"],"rdfs:subClassOf":[{"@id":"d3f:TransferLearning"}]},{"@id":"d3f:HierarchicalClustering","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-HC"],"d3f:definition":["Hierarchical clustering (also called hierarchical cluster analysis or HCA) is a method of cluster analysis that seeks to build a hierarchy of clusters."],"d3f:kb-article":["## References\nWikipedia. (2021, August 10). Hierarchical clustering. [Link](https://en.wikipedia.org/wiki/Hierarchical_clustering)\nhtml)"],"rdfs:hasSubClass":[{"@id":"d3f:AgglomerativeClustering"},{"@id":"d3f:DivisiveClustering"}],"rdfs:label":["Hierarchical Clustering"],"rdfs:subClassOf":[{"@id":"d3f:ClusterAnalysis"}]},{"@id":"d3f:High-dimensionClustering","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-HDC"],"d3f:definition":["The cluster analysis of data with anywhere from a few dozen to many thousands of dimensions."],"d3f:kb-article":["## References\nWikipedia. (n.d.). Clustering high-dimensional data. [Link](https://en.wikipedia.org/wiki/Clustering_high-dimensional_data)"],"rdfs:hasSubClass":[{"@id":"d3f:ProjectedClustering"},{"@id":"d3f:Projection-basedClustering"},{"@id":"d3f:CorrelationClustering"},{"@id":"d3f:Grid-basedClustering"}],"rdfs:label":["High-dimension Clustering"],"rdfs:subClassOf":[{"@id":"d3f:ClusterAnalysis"}]},{"@id":"d3f:Higher-orderLogic","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-HOL"],"d3f:definition":["Higher-order logic is a form of predicate logic that is distinguished from first-order logic by additional quantifiers and, sometimes, stronger semantics. Higher-order logics with their standard semantics are more expressive, but their model-theoretic properties are less well-behaved than those of first-order logic."],"d3f:kb-article":["## References\n1. Higher-order logic. (2023, May 13). In _Wikipedia_. [Link](https://en.wikipedia.org/wiki/Higher-order_logic)"],"d3f:synonym":["HOL"],"rdfs:label":["Higher-order Logic"],"rdfs:subClassOf":[{"@id":"d3f:PredicateLogic"}]},{"@id":"d3f:HomogenousTransferLearning","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-HTL"],"d3f:definition":["In homogeneous transfer learning, the feature spaces of the source and target domains is of the same dimension (Ds = Dt) while the data of both domains is represented by the same attributes (Xs = Xt) and labels (Ys = Yt). Thus, homogeneous transfer learning aims to bridge the gap in the data distributions experienced during cross-domain transfer."],"d3f:kb-article":["## References\nKhalil, K., Asgher, U., & Ayaz, Y. (2022). Novel fNIRS study on homogeneous symmetric feature-based transfer learning for brain-computer interface. Scientific Reports, 12, 3198. [Link](https://www.nature.com/articles/s41598-022-06805-4)."],"rdfs:hasSubClass":[{"@id":"d3f:Instance-basedTransferLearning"},{"@id":"d3f:Parameter-basedTransferLearning"},{"@id":"d3f:Relational-basedTransferLearning"},{"@id":"d3f:AsymmetricFeature-basedTransferLearning"},{"@id":"d3f:SymmetricFeature-basedTransferLearning"},{"@id":"d3f:Hybrid-basedTransferLearning"}],"rdfs:label":["Homogenous Transfer Learning"],"rdfs:subClassOf":[{"@id":"d3f:TransferLearning"}]},{"@id":"d3f:Hybrid-basedTransferLearning","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-HBTL"],"d3f:definition":["This method creates an asymmetric mapping from the target to the source and takes into account bias issues of cross-domain correspondences."],"d3f:kb-article":["## References\nDay, O., & Khoshgoftaar, T.M. (2017). A survey on heterogeneous transfer learning. Journal of Big Data, 4(1), 29. [Link](https://doi.org/10.1186/s40537-017-0089-0)."],"rdfs:label":["Hybrid-based Transfer Learning"],"rdfs:subClassOf":[{"@id":"d3f:HomogenousTransferLearning"}]},{"@id":"d3f:HypothesisTesting","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-HT"],"d3f:definition":["A statistical hypothesis test is a method of statistical inference used to decide whether the data at hand sufficiently support a particular hypothesis. Hypothesis testing allows us to make probabilistic statements about population parameters."],"d3f:kb-article":["## References\nWikipedia. (n.d.). Statistical hypothesis testing. [Link](https://en.wikipedia.org/wiki/Statistical_hypothesis_testing)"],"rdfs:hasSubClass":[{"@id":"d3f:Non-ParametricTests"},{"@id":"d3f:ParametricTests"}],"rdfs:label":["Hypothesis Testing"],"rdfs:subClassOf":[{"@id":"d3f:InferentialStatistics"}]},{"@id":"d3f:ID3","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-ID3"],"d3f:definition":["ID3 stands for Iterative Dichotomiser 3 and is named such because the algorithm iteratively (repeatedly) dichotomizes(divides) features into two or more groups at each step."],"d3f:kb-article":["## Addtional Consiterations\nID3 is the basis of C4.5, and is best used in natural language processing.\n\n## References\nDecision Trees for Classification: ID3 Algorithm Explained. Towards Data Science. [Link](https://towardsdatascience.com/decision-trees-for-classification-id3-algorithm-explained-89df76e72df1)."],"rdfs:label":["ID3"],"rdfs:subClassOf":[{"@id":"d3f:DecisionTree"}]},{"@id":"d3f:Image-to-ImageTranslationGAN","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-ITITG"],"d3f:definition":["Image-to-image translation is the task of transferring styles and characteristics from one image domain to another."],"d3f:kb-article":["## References\nMathWorks. (n.d.). Get Started with GANs for Image-to-Image Translation. [Link](https://www.mathworks.com/help/images/get-started-with-gans-for-image-to-image-translation.html)"],"rdfs:hasSubClass":[{"@id":"d3f:CycleGAN"},{"@id":"d3f:Pix2Pix"}],"rdfs:label":["Image-to-Image Translation GAN"],"rdfs:subClassOf":[{"@id":"d3f:GenerativeAdversarialNetwork"}]},{"@id":"d3f:ImageSynthesisGAN","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-ISG"],"d3f:definition":["Image synthesis thorugh the application of Generative Adversarial Networks."],"d3f:kb-article":["## References\n\nZhang, Q., Wang, H., Lu, H., Won, D., & Yoon, S. W. (2018). Medical Image Synthesis with Generative Adversarial Networks for Tissue Recognition. 2018 IEEE International Conference on Healthcare Informatics (ICHI), 199-207. doi: 10.1109/ICHI.2018.00030. [Link](https://ieeexplore.ieee.org/document/8419363)"],"rdfs:hasSubClass":[{"@id":"d3f:ProgressivelyGrowingGAN"},{"@id":"d3f:StyleGAN"},{"@id":"d3f:DeepConvolutionalGAN"}],"rdfs:label":["Image Synthesis GAN"],"rdfs:subClassOf":[{"@id":"d3f:GenerativeAdversarialNetwork"}]},{"@id":"d3f:InferentialStatistics","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-IS"],"d3f:definition":["Statistical inference is the process of using data analysis to infer properties of an underlying distribution of probability."],"d3f:kb-article":["## References\nWikipedia. (n.d.). Statistical inference. [Link](https://en.wikipedia.org/wiki/Statistical_inference)"],"rdfs:hasSubClass":[{"@id":"d3f:Estimation"},{"@id":"d3f:HypothesisTesting"}],"rdfs:label":["Inferential Statistics"],"rdfs:subClassOf":[{"@id":"d3f:StatisticalMethod"}]},{"@id":"d3f:Instance-basedTransferLearning","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-IBTL"],"d3f:definition":["Instance-based transfer learning methods try to reweight the samples in the source domain in an attempt to correct for marginal distribution differences. These reweighted instances are then directly used in the target domain for training."],"d3f:kb-article":["## References\nGeorgian Impact Blog. (n.d.). Transfer Learning Part 1. [Link](https://medium.com/georgian-impact-blog/transfer-learning-part-1-ed0c174ad6e7#:~:text=Homogeneous%20Transfer%20Learning-,1.,the%20target%20domain%20for%20training)."],"rdfs:label":["Instance-based Transfer Learning"],"rdfs:subClassOf":[{"@id":"d3f:HomogenousTransferLearning"}]},{"@id":"d3f:InterquartileRange","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-IR"],"d3f:definition":["The interquartile range (IQR) is a measure of statistical dispersion, which is the spread of the data and is defined as the difference between the 75th and 25th percentiles of the data."],"d3f:kb-article":["## References\nWikipedia. (n.d.). Interquartile range. [Link](https://en.wikipedia.org/wiki/Interquartile_range)"],"d3f:synonym":["IQR"],"rdfs:label":["Interquartile Range"],"rdfs:subClassOf":[{"@id":"d3f:Variability"}]},{"@id":"d3f:IntervalEstimation","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-IE"],"d3f:definition":["Interval estimation is the use of sample data to estimate an interval of possible values of a parameter of interest."],"d3f:kb-article":["## References\nWikipedia. (n.d.). Interval estimation. [Link](https://en.wikipedia.org/wiki/Interval_estimation)"],"rdfs:label":["Interval Estimation"],"rdfs:subClassOf":[{"@id":"d3f:Estimation"}]},{"@id":"d3f:IntrinsicallySemi-supervisedLearning","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-ISSL"],"d3f:definition":["These methods directly optimize an objective function with components for labeled and unlabeled samples and do not rely on any intermediate steps or supervised base learners. Basically, these methods are extension of existing supervised methods to include the effect of unlabeled data samples in the objective function."],"d3f:kb-article":["## References\nBeginner's Guide to Semi-Supervised Learning. Jashish Blog.  [Link](http://jashish.com.np/blog/posts/beginners-guide-to-semi-supervised-learning/)."],"rdfs:hasSubClass":[{"@id":"d3f:Semi-supervisedGenerativeModelLearning"},{"@id":"d3f:Semi-supervisedManifoldLearning"},{"@id":"d3f:Maximum-marginLearning"},{"@id":"d3f:Perturbation-basedLearning"}],"rdfs:label":["Intrinsically Semi-supervised Learning"],"rdfs:subClassOf":[{"@id":"d3f:Semi-SupervisedLearning"}]},{"@id":"d3f:K-CenterClustering","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-KCC"],"d3f:definition":["K-center Clustering is a type of clustering based on an combinatorial optimization methods.  It clusters a set of points so as to minimize the maximum intercluster distance."],"d3f:kb-article":["## How it works\n\nAn example K-center Clustering problem is to mimimize the number of points in a set that are necessary so that a every other point in the set is within some fixed distance of those points.  For instance, given n cities with specified distances, one wants to build k warehouses in different cities and minimize the maximum distance of a city to a warehouse.\n\n## Considerations\n\n- **Scalability**: Exact solutions are NP-hard.  However, algorithms\n    that have been proven effective and create no more than 2x the\n    optimal set of clusters can run in O(kn) proportional to k*n where\n    k is the minimum number of clusters and n is the number of data\n    points being clustered.\n\n## Key Test Considerations\n\n- **Unsupervised Learning**:\n\n  - **Number of Clusters**: The Gonzalez (Gon) algorithm guarantees\n      creating no more than twice the optimal number of clusters,\n      where the optimal number is the minimum number of clusters to\n      minimize total distance between representative points in the\n      clusters [1].\n\n- **Cluster Analysis**:\n\n    - **Rand Index and Adjusted Rand Index**: Given ground truth set\n      of class labels for the data, the Rand Index is a measure of the\n      similarity between two data clusterings. The Rand Index is the\n      accuracy of determining if a link belongs within a cluster or\n      not. A form of the Rand Index may be defined that is adjusted\n      for the chance grouping of elements, this is the Adjusted Rand\n      Index [5].\n\n    - **Adjusted Mutual Information**: Given ground truth set of class\n      labels for the data, Adjusted Mutual Information corrects the\n      effect of agreement solely due to chance between clusterings,\n      similar to the way the Adjusted Rand Index corrects the Rand\n      Index [6].\n\n- **Connection-based Clustering**:\n\n  - **Choice of Distance Metric**: The outcome can vary significantly depending on the chosen distance metric (e.g., Euclidean, Manhattan).\n\n  - **Sensitivity**: Connection-based method can be sensitive to outliers, which might affect the quality of the clusters formed.\n\n- **K-center Clustering**:\n\n  - **Silhouette Score**: The silhouette score refers to a scoring\n    method that helps validate the consistency between clusters of\n    data. The evaluation technique also produces a concise graphical\n    representation of how well each object appear to have been\n    classified.  It is suited to K-centric Clustering in that it also\n    works for different metric spaces.\n\n  - **Distance Metric**: The distance measure must be a true metric (see\n    [2]).  Differences in the metric chosen may (e.g., Euclidean,a\n    Manhattan) affect results significantly.\n\n  - **Sensitivity**: Greedy implementations may be sensitive to\n    outliers.\n\n## Platforms, Tools, or Libraries\n\nN/A. _Note that this algorithm is relatively simple and so it is\nusually implemented from scratch by those incorporating this algorithm\ninto a system._\n\n## References\n\n1. Gonzalez, T.F. (1985). Clustering to Minimize the Maximum Intercluster Distance. Theor. Comput. Sci., 38, 293-306.\n[Link](https://www.sciencedirect.com/science/article/pii/0304397585902245?via%3Dihub).\n\n1. Weisstein, Eric W. (n.d.). \"Metric.\" From MathWorld--A Wolfram Web Resource. [Link](https://mathworld.wolfram.com/Metric.html).\n\n1. Wikipedia. (8 Aug 2023). Metric k-center [Link](https://en.wikipedia.org/wiki/Metric_k-center).\n\n1. Wikipedia. (14 Aug 2023). Vertex k-center problem. [Link](https://en.wikipedia.org/wiki/Vertex_k-center_problem).\n\n1. Wikipedia. (n.d.). Rand Index. [Link](https://en.wikipedia.org/wiki/Rand_index).\n\n1. Wikipedia. (n.d.). Adjusted Mutual Information. [Link](https://en.wikipedia.org/wiki/Adjusted_mutual_information).\n\n1. Wikipedia. (1 Aug 2023). Silhouette (clustering). [Link](https://en.wikipedia.org/wiki/Silhouette_(clustering))."],"rdfs:label":["K-Center Clustering"],"rdfs:subClassOf":[{"@id":"d3f:Graph-basedClustering"}]},{"@id":"d3f:K-FoldCross-Validation","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-KFCV"],"d3f:definition":["Cross-validation is a resampling procedure used to evaluate machine learning models on a limited data sample. The procedure has a single parameter called k that refers to the number of groups that a given data sample is to be split into. As such, the procedure is often called k-fold cross-validation. When a specific value for k is chosen, it may be used in place of k in the reference to the model, such as k=10 becoming 10-fold cross-validation"],"d3f:kb-article":["## References\nK-Fold Cross-Validation. Machine Learning Mastery.  [Link](https://machinelearningmastery.com/k-fold-cross-validation/#:~:text=Cross%2Dvalidation%20is%20a%20resampling,k%2Dfold%20cross%2Dvalidation)."],"rdfs:label":["K-Fold Cross-Validation"],"rdfs:subClassOf":[{"@id":"d3f:ResamplingEnsemble"}]},{"@id":"d3f:K-NearestNeighbors","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-KNN"],"d3f:definition":["The k-nearest neighbors algorithm, also known as KNN or k-NN, is a non-parametric, supervised learning classifier, which uses proximity to make classifications or predictions about the grouping of an individual data point."],"d3f:kb-article":["## **How it works**\nThe goal of the k-nearest neighbor algorithm is to identify the nearest neighbors of a given query point, so that we can assign a class label to that point. To determine which data points are closest to a given query point, the distance between the query point and the other data points will need to be calculated. The distance measures used can vary depending on the data set or implementation and help inform decision boundaries, which query points into different regions. Then, by defining the k-value (the number of neighbors to be checked to determine the classification of a specific query point), the data can be assigned its class label.\n\nFor classification problems, a class label is assigned on the basis of a majority vote—i.e. the label that is most frequently represented around a given data point is used (the term “majority vote” is commonly used in literature, however, the technique is more technically considered “plurality voting”).  Regression problems use a similar concept as classification problem, but in this case, the average the k nearest neighbors is taken to make a prediction about a classification. The main distinction here is that classification is used for discrete values, whereas regression is used with continuous ones.\n\nUnlike other algorithms that explicitly model the problem, such as linear regression, KNN is instance-based. It means that the algorithm doesn't explicitly learn a model. Instead, it memorizes the training instances and uses them as \"knowledge\" for the prediction phase. It's also worth noting that the KNN algorithm is also part of a family of “lazy learning” models, meaning that it only stores a training dataset versus undergoing a training stage.\n\n## **Considerations**\n\n* **Scaling:** Scaling is a problem as KNN is a lazy algorithm and takes up more memory and storage compared to other classification methods.\n\n* **Implementation and Hyperparameters:** As KNN only requires a k-value and a distance metric, it is often an easy implementation and can adjust will to new training data.\n\n## Key Test Considerations\n\n- **Supervised Learning:**\n\n  - **Cross Validation:** As cross validation methods like k-fold, leave-one-out, and stratified cross validation can help validate model performance. However, nuances like pessimism bias in k-fold cross validation or high variability in leave-one-out cross validation may need consideration.\n\n- **Classification:**\n\n  - **ROC Curve:**  A standard technique used to summarize classifier performance over a range of tradeoffs between true and false positives is the Receiver Operating Characteristic (ROC) curve.\n\n  - **Data Imbalance:** Imbalanced data sets where one class significantly outnumbers others, under sampling techniques like SMOTE may be beneficial in sampling minority classes.\n\n- **K-Nearest Neighbor**\n\n  - **Choice of K:** The number of neighbors, K, affects the decision boundary. A smaller K can lead to a noisy decision boundary, while a large K can smooth it out, but may also blur class distinctions.\n\n  - **K-d Tree:** Exact searching on large datasets can be computationally costly and inefficient. Implementing approximate nearest neighbor algorithms like the K-d tree algorithm.\n\n  - **Dimensionality:** KNN does not perform well while using high-dimensional data and can be sensitive to irrelevant features which can lead to overfitting.\n\n  - **Distance Metric:** Choosing the appropriate distance metric (Euclidean, Manhattan, MinKowski, Hamming etc.) is essential, based on the nature of the data.\n\n## **References**\n1. IBM. K-Nearest Neighbors Algorithm.  [Link](https://www.ibm.com/topics/knn?mhsrc=ibmsearch_a&mhq=k-nearest%20neighbors%20).\n2. Muja, M., & Lowe, D. G. (2014). Scalable nearest neighbor algorithms for high dimensional data. IEEE Transactions on Pattern Analysis and Machine Intelligence. [Link]( https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6809191).\n3. Chawla, N. V., Bowyer, K. W., Hall, L. O., & Kegelmeyer, W. P. (2002). SMOTE: synthetic minority over-sampling technique. Journal of artificial intelligence research, 16, 321-357. [Link]( https://www.jair.org/index.php/jair/article/view/10302/24590).\n4. Kohavi, R. (1995). A study of cross-validation and bootstrap for accuracy estimation and model selection. Proceedings of the 14th international joint conference on Artificial intelligence . [Link]( https://www.ijcai.org/Proceedings/95-2/Papers/016.pdf)."],"rdfs:label":["K-Nearest Neighbors"],"rdfs:subClassOf":[{"@id":"d3f:Classification"}]},{"@id":"d3f:K-meansClustering","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-KMC"],"d3f:definition":["K-means algorithm identifies k number of centroids, and then allocates every data point to the nearest cluster, while keeping the centroids as small as possible."],"d3f:kb-article":["## References\nTowards Data Science. (n.d.). Understanding K-means Clustering in Machine Learning. [Link](https://towardsdatascience.com/understanding-k-means-clustering-in-machine-learning-6a6e67336aa1)"],"rdfs:label":["K-means Clustering"],"rdfs:subClassOf":[{"@id":"d3f:Centroid-basedClustering"}]},{"@id":"d3f:KendallsRankCorrelationCoefficient","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-KRCC"],"d3f:definition":["Kendall's $\\\\tau$ between and is given by $(n_c - n_d) / \\\\sqrt((n_c+n_d+n_x)(n_c+n_d+n_y)$, where is the number of concordant pairs of observations, is the number of discordant pairs, is the number of ties involving only the variable, and is the number of ties involving only the variable.\" ;"],"d3f:kb-article":["## References\n1. Wolfram Research. (2012). KendallTau. Wolfram Language function.  [Link](https://reference.wolfram.com/language/ref/KendallTau.html)\n1. Kendall's Tau. (2023, May 23). In _Wikipedia_. [Link](https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient]\"\"\","],"d3f:synonym":["Kendall's Tau Coefficient"],"rdfs:isDefinedBy":["https://reference.wolfram.com/language/ref/KendallTau.html"],"rdfs:label":["Kendall's Rank Correlation Coefficient"],"rdfs:subClassOf":[{"@id":"d3f:RankCorrelationCoefficient"}]},{"@id":"d3f:Kurtosis","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-KUR"],"d3f:definition":["The measure of the \"fatness\" of the tails of a pmf or pdf. The fourth standardized moment of the distribution."],"d3f:kb-article":["## References\nWikipedia. (n.d.). Probability distribution. [Link](https://en.wikipedia.org/wiki/Probability_distribution)"],"rdfs:label":["Kurtosis"],"rdfs:subClassOf":[{"@id":"d3f:DistributionProperties"}]},{"@id":"d3f:LevenshteinMatching","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-LM"],"d3f:definition":["The Levenshtein distance (LD) is a metric for measuring the differences between two sequences - or strings. Informally, the LD is the number of individual edits one would have to make to turn one sequence into another."],"d3f:kb-article":["## References\n1. Navarro, G. (2001). A guided tour to approximate string matching. _ACM Computing Surveys_, 33(1), 31-88. [Link](https://doi.org/10.1145/375360.375365)"],"d3f:synonym":["Edit Distance"],"rdfs:label":["Levenschtein Matching"],"rdfs:subClassOf":[{"@id":"d3f:ApproximateStringMatching"}]},{"@id":"d3f:LinearClassifier","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-LC"],"d3f:definition":["A linear classifier is a model that makes a decision to categories a set of data points to a discrete class based on a linear combination of its explanatory variables"],"d3f:kb-article":["## References\nA Look at the Maths Behind Linear Classification. Towards Data Science. [Link](https://towardsdatascience.com/a-look-at-the-maths-behind-linear-classification-166e99a9e5fb)."],"rdfs:label":["Linear Classifier"],"rdfs:subClassOf":[{"@id":"d3f:Classification"}]},{"@id":"d3f:LinearLogicProgramming","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-LLP"],"d3f:definition":["Linear logic programming is a form of logic programming that uses linear logic, that is, it emphasizes the use of formulas as resources."],"d3f:kb-article":["## References\n1. Cosmo, R. and Miller D. (2019, May 24). _Linear logic_. Stanford Encyclopedia of Philosophy. [Link](https://plato.stanford.edu/entries/logic-linear/#LinLogComSci)\n2. Linear logic programming. (2023, May 16). In _Wikipedia_. [Link](https://en.wikipedia.org/wiki/Logic_programming#Linear_logic_programming)"],"rdfs:label":["Linear Logic Programming"],"rdfs:subClassOf":[{"@id":"d3f:LogicProgramming"}]},{"@id":"d3f:LinearRegression","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-LR"],"d3f:definition":["Statistical regression method used for predictive analysis by modeling the linear relationship between independent and dependent variables."],"d3f:kb-article":["## How it works\nTakes independent variables (i.e. covariate, features, predictors, input variables) and dependent variables (i.e. response, output, “thing to be estimated”) and produces the coefficient(s) and intercept for a linear equation (e.g. (β1, β0) for y = β1x + β0) which predicts the relationship between the independent and independent variables by minimizing a cost function, Mean Squared Error, either directly in the case of univariate linear regression or by gradient descent* in the case of multivariate linear regression.\n\n## Considerations\n - There are four principal assumptions required for good results using linear regression (the first letters of the four principal assumptions form the \"LINE\" mnemonic):\n   - Linearity and Additivity\n   - Independent Residuals\n   - Normal Residual Distributions\n   - Equal Variances (i.e. homoscedasticity)\n - Linear regression is a low variance/high bias model.\n - Optimizers like Adam, Batch, and Mini-Batch and others are available for certain applications and data sets.\n- A large learning ratio or training coefficient may lead to divergent behavior of the model and too small of values may lead to long run times and inefficiency.\n\n\n## Verification Approach\n - Models are often evaluated by examining one or more of the metrics of R2, Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and Mean Absolute Percentage Error (MAPE).\n - While there is no generally accepted single best performance metric as a criterion, users of linear regression should consider the suitability of one or more of these metrics for assessing the performance of their model.\n - Use well known data sets to verify model execution.\n\n## Validation Approach\n - Violating the principal assumptions of linear regression results in poor or misleading results.\n - Ensure data is truly representative and if there are any known biases.\n\n\n## References\n1. Gawali, Suvarna. “Linear Regression Algorithm to Make Predictions Easily.” Analytics Vidhya, 22 July 2022, https://www.analyticsvidhya.com/blog/2021/06/linear-regression-in-machine-learning/.\n1. Nau, Robert. “Statistical Forecasting: Notes On Regression and Time Series Analysis.” Introduction to Linear Regression Analysis, Duke University Fuqua School of Business, 18 Aug. 2020, https://people.duke.edu/~rnau/regintro.htm.\n1. Ng, Ritchie. “Evaluating a Linear Regression Model.” Ritchieng.github.io, 8 Jan. 2023, https://www.ritchieng.com/machine-learning-evaluate-linear-regression-model/.\n1. Bochkarev, Alexei. \"A New Typology Design of Performance Metrics to Measure Errors in Machine Learning Regression Algorithms\", 2019, https://www.researchgate.net/publication/330661543_A_New_Typology_Design_of_Performance_Metrics_to_Measure_Errors_in_Machine_Learning_Regression_Algorithms."],"rdfs:label":["Linear Regression"],"rdfs:subClassOf":[{"@id":"d3f:RegressionAnalysis"}]},{"@id":"d3f:LinearRegressionLearning","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-LRL"],"d3f:definition":["A supervised learning method that builds a linear regression model using training data."],"d3f:kb-article":["## References\n- Gawali, Suvarna. “Linear Regression Algorithm to Make Predictions Easily.” Analytics Vidhya, 22 July 2022, https://www.analyticsvidhya.com/blog/2021/06/linear-regression-in-machine-learning/.\n- Nau, Robert. “Statistical Forecasting: Notes On Regression and Time Series Analysis.” Introduction to Linear Regression Analysis, Duke University Fuqua School of Business, 18 Aug. 2020, https://people.duke.edu/~rnau/regintro.htm.\n- Ng, Ritchie. “Evaluating a Linear Regression Model.” Ritchieng.github.io, 8 Jan. 2023, https://www.ritchieng.com/machine-learning-evaluate-linear-regression-model/.\n- Bochkarev, Alexei. \"A New Typology Design of Performance Metrics to Measure Errors in Machine Learning Regression Algorithms\", 2019, https://www.researchgate.net/publication/330661543_A_New_Typology_Design_of_Performance_Metrics_to_Measure_Errors_in_Machine_Learning_Regression_Algorithms."],"rdfs:label":["Linear Regression Learning"],"rdfs:seeAlso":["http://d3fend.mitre.org/ontologies/d3fend.owl#LinearRegression"],"rdfs:subClassOf":[{"@id":"d3f:RegressionAnalysisLearning"}]},{"@id":"d3f:LogicProgramming","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-LP"],"d3f:definition":["Logic programming is a programming paradigm which is largely based on formal logic."],"d3f:kb-article":["## How it works\nAny program written in a logic programming language is a set of sentences in logical form, expressing facts and rules about some problem domain. Major logic programming language families include Prolog, answer set programming (ASP) and Datalog. In all of these languages, rules are written in the form of clauses:\n\nH :- B_1, ..., B_n.\n\n## References\n1. Logic programming. (2023, May 29). In _Wikipedia_. [Link]( https://en.wikipedia.org/wiki/Logic_programming)"],"rdfs:hasSubClass":[{"@id":"d3f:AnswerSetProgramming"},{"@id":"d3f:Prolog"},{"@id":"d3f:LinearLogicProgramming"},{"@id":"d3f:Datalog"}],"rdfs:label":["Logic Programming"],"rdfs:subClassOf":[{"@id":"d3f:SymbolicAI"}]},{"@id":"d3f:LogicalRules","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-LR"],"d3f:definition":["A logical rule matches event data or set of values to a conditional expression and results in the determination of a truth value, which may be used to determine the next action or step to take."],"d3f:kb-article":["## How it works\n\nLogic rules define a set of patterns that in some patterns must match input data. If the the conditions are met, then the rule will \"fire\" and some action will be taken, usually notifying a person or another system that the event being monitored needs further processing or attention.\n\n## Key Test Considerations\n\n- **Performance (Accuracy)** Identify instances in data where rule is expected to be triggered. Implement traceability and metrics for individual rule performance. Traceability of cases could be implemented as unit tests or as part of a fine-grained classification performance platform. For simple rule-based matching systems with many rules, individual rules may be unused or may create unusually high false positives (or false negatives relative to expectation.\n\n- **Performance (Computational)** Generate model performance measures (see Classification Performance Measures), esp. a confusion matrix for each rule and identify outliers and relative contribution of rule to overall performance.\n\n## References\n1. Event condition action. (2019, Nov 21). In _Wikipedia_. [Link](https://en.wikipedia.org/wiki/Event_condition_action).\n2. Business rule. (2023, April 10). In _Wikipedia_. [Link](https://en.wikipedia.org/wiki/Business_rule).\n3. YARA. (2023, June 5). In _Wikipedia_. [Link](https://en.wikipedia.org/wiki/YARA)."],"rdfs:hasSubClass":[{"@id":"d3f:BooleanExpressionMatching"},{"@id":"d3f:EquivalenceMatching"},{"@id":"d3f:PatternMatching"}],"rdfs:label":["Logical Rules"],"rdfs:subClassOf":[{"@id":"d3f:SymbolicLogic"}]},{"@id":"d3f:LogisticRegression","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-LR"],"d3f:definition":["Logistic regression is estimating the parameters of a logistic model."],"d3f:kb-article":["## References\nWikipedia. (n.d.). Logistic regression. [Link](https://en.wikipedia.org/wiki/Logistic_regression)"],"rdfs:label":["Logistic Regression"],"rdfs:subClassOf":[{"@id":"d3f:RegressionAnalysis"}]},{"@id":"d3f:LogististicRegressionLearning","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-LRL"],"d3f:definition":["A supervised learning method that builds a logistic regression model using training data."],"d3f:kb-article":["## References\nWikipedia. (n.d.). Logistic regression. [Link](https://en.wikipedia.org/wiki/Logistic_regression)"],"rdfs:label":["Logistic Regression Learning"],"rdfs:seeAlso":["http://d3fend.mitre.org/ontologies/d3fend.owl#LogisticRegression"],"rdfs:subClassOf":[{"@id":"d3f:RegressionAnalysisLearning"}]},{"@id":"d3f:LongShort-termMemory","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-LSTM"],"d3f:definition":["Unlike standard feedforward neural networks, LSTM has feedback connections. Such a recurrent neural network (RNN) can process not only single data points (such as images), but also entire sequences of data (such as speech or video). This characteristic makes LSTM networks ideal for processing and predicting data"],"d3f:kb-article":["## References\nWikipedia. (2021, September 29). Long short-term memory. [Link](https://en.wikipedia.org/wiki/Long_short-term_memory)"],"rdfs:label":["Long Short-term Memory"],"rdfs:subClassOf":[{"@id":"d3f:RecurrentNeuralNetwork"}]},{"@id":"d3f:MachineLearning","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-ML"],"d3f:definition":["Machine learning techniques are computational methods that combine statistics, probability, and optimization to make accurate predictions and/or improve performance."],"d3f:kb-article":["## References\nMachine learning.\" Wikipedia. [Link](https://en.wikipedia.org/wiki/Machine_learning)."],"rdfs:hasSubClass":[{"@id":"d3f:TransferLearning"},{"@id":"d3f:SupervisedLearning"},{"@id":"d3f:UnsupervisedLearning"},{"@id":"d3f:ActiveLearning"},{"@id":"d3f:EnsembleLearning"},{"@id":"d3f:ReinforcementLearning"},{"@id":"d3f:Semi-SupervisedLearning"},{"@id":"d3f:Transformer-basedLearning"}],"rdfs:label":["Machine Learning"],"rdfs:subClassOf":[{"@id":"d3f:AnalyticTechnique"}]},{"@id":"d3f:Maximum-marginLearning","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-MML"],"d3f:definition":["Maximum-margin classifiers attempt to maximize the distance between the given data points and the decision boundary"],"d3f:kb-article":["## References\nEngelen, S., & Hoos, H. (2020). A survey on semi-supervised learning. Machine Learning, 109(2), 299-337. [Link](https://link.springer.com/article/10.1007/s10994-019-05855-6).\n\nSupport Vector Machines for Machine Learning. [Link](https://machinelearningmastery.com/support-vector-machines-for-machine-learning/#:~:text=The%20distance%20between%20the%20line,called%20the%20Maximal%2DMargin%20hyperplane.)"],"rdfs:label":["Maximum-margin Learning"],"rdfs:subClassOf":[{"@id":"d3f:IntrinsicallySemi-supervisedLearning"}]},{"@id":"d3f:Mean","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-MEA"],"d3f:definition":["The sum of all measurements divided by the number of observations in the data set."],"d3f:kb-article":["## References\nWikipedia. (n.d.). Central tendency. [Link](https://en.wikipedia.org/wiki/Central_tendency)"],"rdfs:label":["Mean"],"rdfs:subClassOf":[{"@id":"d3f:CentralTendency"}]},{"@id":"d3f:MeanAbsoluteDeviation","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-MAD"],"d3f:definition":["The mean absolute deviation (MAD), also referred to as the \"mean deviation\" or sometimes \"average absolute deviation\", is the mean of the data's absolute deviations around the data's mean: the average (absolute) distance from the mean."],"d3f:kb-article":["## References\nWikipedia. (n.d.). Average absolute deviation. [Link](https://en.wikipedia.org/wiki/Average_absolute_deviation)"],"d3f:synonym":["MAD"],"rdfs:label":["Mean Absolute Deviation"],"rdfs:subClassOf":[{"@id":"d3f:AverageAbsoluteDeviation"}]},{"@id":"d3f:Median","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-MED"],"d3f:definition":["The middle value that separates the higher half from the lower half of the data set. The median and the mode are the only measures of central tendency that can be used for ordinal data, in which values are ranked relative to each other but are not measured absolutely."],"d3f:kb-article":["## References\nWikipedia. (n.d.). Central tendency. [Link](https://en.wikipedia.org/wiki/Central_tendency)"],"rdfs:label":["Median"],"rdfs:subClassOf":[{"@id":"d3f:CentralTendency"}]},{"@id":"d3f:MedianAbsoluteDeviation","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-MAD"],"d3f:definition":["The median absolute deviation (also MAD) is the median of the absolute deviation from the median."],"d3f:kb-article":["## References\nWikipedia. (n.d.). Average absolute deviation. [Link](https://en.wikipedia.org/wiki/Average_absolute_deviation)"],"rdfs:label":["Median Absolute Deviation"],"rdfs:subClassOf":[{"@id":"d3f:AverageAbsoluteDeviation"}]},{"@id":"d3f:ModalLogic","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-ML"],"d3f:definition":["Modal logic is a collection of formal systems developed to represent statements about necessity and possibility. It plays a major role in philosophy of language, epistemology, metaphysics, and natural language semantics."],"d3f:kb-article":["## References\n1. Modal logic. (2023, June 4). In _Wikipedia_. [Link](https://en.wikipedia.org/wiki/Modal_logic)"],"rdfs:hasSubClass":[{"@id":"d3f:AlethicLogic"},{"@id":"d3f:DeonticLogic"},{"@id":"d3f:EpistemicLogic"},{"@id":"d3f:TemporalLogic"}],"rdfs:label":["Modal Logic"],"rdfs:subClassOf":[{"@id":"d3f:SymbolicAI"}]},{"@id":"d3f:Mode","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-MOD"],"d3f:definition":["The most frequent value in the data set. This is the only central tendency measure that can be used with nominal data, which have purely qualitative category assignments."],"d3f:kb-article":["## References\nWikipedia. (n.d.). Central tendency. [Link](https://en.wikipedia.org/wiki/Central_tendency)"],"rdfs:label":["Mode"],"rdfs:subClassOf":[{"@id":"d3f:CentralTendency"}]},{"@id":"d3f:Model-basedPolicyOptimization","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-MBPO"],"d3f:definition":["Model-based policy optimization (MBPO) is a model-based, online, off-policy reinforcement learning algorithm. For more information on the different types of reinforcement learning agents"],"d3f:kb-article":["## References\nMBPO Agents. MathWorks.  [Link](https://www.mathworks.com/help/reinforcement-learning/ug/mbpo-agents.html)."],"rdfs:label":["Model-based Policy Optimization"],"rdfs:subClassOf":[{"@id":"d3f:Model-basedReinforcementLearning"}]},{"@id":"d3f:Model-basedReinforcementLearning","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-MBRL"],"d3f:definition":["Model-based Reinforcement Learning refers to learning optimal behavior indirectly by learning a model of the environment by taking actions and observing the outcomes that include the next state and the immediate reward. The models predict the outcomes of actions and are used in lieu of or in addition to interaction with the environment to learn optimal policies."],"d3f:kb-article":["## References\nModel-Based Reinforcement Learning. In *Encyclopedia of Machine Learning*, pp. 642-644. Springer, 2010.  [Link](https://link.springer.com/referenceworkentry/10.1007/978-0-387-30164-8_556#:~:text=Model%2Dbased%20Reinforcement%20Learning%20refers,state%20and%20the%20immediate%20reward)."],"rdfs:hasSubClass":[{"@id":"d3f:Dyna-Q"},{"@id":"d3f:Model-basedPolicyOptimization"},{"@id":"d3f:Model-basedValueIteration"}],"rdfs:label":["Model-based Reinforcement Learning"],"rdfs:subClassOf":[{"@id":"d3f:ReinforcementLearning"}]},{"@id":"d3f:Model-basedValueIteration","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-MBVI"],"d3f:definition":["Value Iteration effectively reducesthe evaluation stage down to a single sweep of the states. Additionally, to improve things further, it combines the Policy Evaluation and Policy Improvement stages into a single update."],"d3f:kb-article":["## References\nPolicy and Value Iteration. Towards Data Science.  [Link](https://towardsdatascience.com/policy-and-value-iteration-78501afb41d2)."],"d3f:synonym":["MBVI"],"rdfs:label":["Model-based Value Iteration"],"rdfs:subClassOf":[{"@id":"d3f:Model-basedReinforcementLearning"}]},{"@id":"d3f:Model-freeReinforcementLearning","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-MFRL"],"d3f:definition":["In reinforcement learning (RL), a model-free algorithm (as opposed to a model-based one) is an algorithm which does not use the transition probability distribution (and the reward function) associated with the Markov decision process (MDP),which, in RL, represents the problem to be solved. The transition probability distribution (or transition model) and the reward function are often collectively called the \"model\" of the environment (or MDP), hence the name \"model-free\". A model-free RL algorithm can be thought of as an \"explicit\" trial-and-error algorithm. An example of a model-free algorithm is Q-learning."],"d3f:kb-article":["## References\nModel-free (reinforcement learning). Wikipedia. [Link](https://en.wikipedia.org/wiki/Model-free_(reinforcement_learning)).)"],"rdfs:hasSubClass":[{"@id":"d3f:Q-Learning"},{"@id":"d3f:TemporalDifferenceLearning"},{"@id":"d3f:PolicyGradient"},{"@id":"d3f:SARSA"}],"rdfs:label":["Model-free Reinforcement Learning"],"rdfs:subClassOf":[{"@id":"d3f:ReinforcementLearning"}]},{"@id":"d3f:Moments","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-MOM"],"d3f:definition":["With a probability distribution function, the the first moment is the expected value, the second central moment is the variance, the third standardized moment is the skewness, and the fourth standardized moment is the kurtosis."],"d3f:kb-article":["## References\nWikipedia. (n.d.). Moment (mathematics). [Link](https://en.wikipedia.org/wiki/Moment_(mathematics))"],"rdfs:label":["Moments"],"rdfs:subClassOf":[{"@id":"d3f:DistributionProperties"}]},{"@id":"d3f:MovingAverageModel","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-MAM"],"d3f:definition":["the moving-average model (MA model) is an approach for modeling univariate time series and specifies that the output variable is cross-correlated with a non-identical to itself random-variable."],"d3f:kb-article":["## Refrences\nWikipedia. (n.d.). Moving average model. [Link](https://en.wikipedia.org/wiki/Moving_average_model)"],"d3f:synonym":["MA Model"],"rdfs:label":["Moving Average Model"],"rdfs:subClassOf":[{"@id":"d3f:TimeSeriesAnalysis"}]},{"@id":"d3f:MultilayerPerceptronClassification","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-MPC"],"d3f:definition":["A multilayer perceptron (MLP) is a fully connected class of feedforward artificial neural network (ANN).An MLP consists of at least three layers of nodes: an input layer, a hidden layer and an output layer."],"d3f:kb-article":["## References\nMultilayer perceptron. Wikipedia. [Link](https://en.wikipedia.org/wiki/Multilayer_perceptron)."],"rdfs:label":["Multilayer Perceptron Classification"],"rdfs:subClassOf":[{"@id":"d3f:ArtificialNeuralNetClassification"}]},{"@id":"d3f:MultipleRegression","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-MR"],"d3f:definition":["Multiple (linear) regression attempts to model the relationship between two or more explanatory variables and a response variable by fitting a linear equation to observed data."],"d3f:kb-article":["## References\nYale University Department of Statistics. (1997-98). Linear regression and multivariate analysis. [Link](http://www.stat.yale.edu/Courses/1997-98/101/linmult.htm)"],"d3f:synonym":["Multiple Linear Regression"],"rdfs:label":["Multiple Regression"],"rdfs:subClassOf":[{"@id":"d3f:RegressionAnalysis"}]},{"@id":"d3f:MultipleRegressionLearning","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-MRL"],"d3f:definition":["A supervised learning method that builds a multiple regression model using training data."],"d3f:kb-article":["## References\nYale University Department of Statistics. (1997-98). Linear regression and multivariate analysis. [Link](http://www.stat.yale.edu/Courses/1997-98/101/linmult.htm)"],"rdfs:label":["Multiple Regression Learning"],"rdfs:seeAlso":["http://d3fend.mitre.org/ontologies/d3fend.owl#MultipleRegression"],"rdfs:subClassOf":[{"@id":"d3f:RegressionAnalysisLearning"}]},{"@id":"d3f:MultivariateAnalysis","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-MA"],"d3f:definition":["Multivariate statistics encompassed the simultaneous observation and analysis of more than one outcome variable, i.e., multivariate random variables."],"d3f:kb-article":["## References\nWikipedia. (n.d.). Multivariate statistics. [Link](https://en.wikipedia.org/wiki/Multivariate_statistics)"],"rdfs:hasSubClass":[{"@id":"d3f:DiscriminantAnalysis"},{"@id":"d3f:PrincipalComponentAnalysis"}],"rdfs:label":["Multivariate Analysis"],"rdfs:subClassOf":[{"@id":"d3f:StatisticalMethod"}]},{"@id":"d3f:NaiveBayesClassifier","@type":["owl:Class","owl:NamedIndividual"],"d3f:d3fend-id":["D3A-NBC"],"d3f:definition":["The Naïve Bayes classifier is a supervised machine learning algorithm, which is used for classification tasks, like text classification. It is also part of a family of generative learning algorithms, meaning that it seeks to model the distribution of inputs of a given class or category."],"d3f:kb-article":["## References\nNaive Bayes. IBM. [Link](https://www.ibm.com/topics/naive-bayes?mhsrc=ibmsearch_a&mhq=naive%20bayes)."],"rdfs:label":["Naive Bayes Classifier"],"rdfs:subClassOf":[{"@id":"d3f:Classification"}]},{"@id":"d3f:Non-ParametricTests","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-NPT"],"d3f:definition":["A non-parametric test relies is used when the underlying distribution of data is non-symmetric (non-normal distribution)."],"d3f:kb-article":["## References\nNewcastle University. (n.d.). Parametric Hypothesis Tests. [Link](https://www.ncl.ac.uk/webtemplate/ask-assets/external/maths-resources/psychology/non-parametric-hypothesis-tests.html)"],"rdfs:label":["Non-Parametric Tests"],"rdfs:subClassOf":[{"@id":"d3f:HypothesisTesting"}]},{"@id":"d3f:Non-monotonicLogic","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-NML"],"d3f:definition":["Non-monotonic logic is a formal logic whose conclusion relation is not monotonic. In other words, non-monotonic logics are devised to capture and represent defeasible inferences (cf. defeasible reasoning), i.e., a kind of inference in which reasoners draw tentative conclusions, enabling reasoners to retract their conclusion(s) based on further evidence."],"d3f:kb-article":["## References\n1. Non-monotonic logic. (2023, June 1). In _Wikipedia_. [Link](https://en.wikipedia.org/wiki/Non-monotonic_logic)"],"rdfs:label":["Non-monotonic Logic"],"rdfs:subClassOf":[{"@id":"d3f:SymbolicAI"}]},{"@id":"d3f:NonlinearRegression","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-NR"],"d3f:definition":["Nonlinear regression is a form of regression analysis in which observational data are modeled by a function which is a nonlinear combination of the model parameters and depends on one or more independent variables."],"d3f:kb-article":["## References\nWikipedia. (n.d.). Nonlinear regression. [Link](https://en.wikipedia.org/wiki/Nonlinear_regression)"],"rdfs:label":["Nonlinear Regression"],"rdfs:subClassOf":[{"@id":"d3f:RegressionAnalysis"}]},{"@id":"d3f:NonlinearRegressionLearning","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-NRL"],"d3f:definition":["A supervised learning method that builds a non-linear regression model using training data."],"d3f:kb-article":["## References\nWikipedia. (n.d.). Nonlinear regression. [Link](https://en.wikipedia.org/wiki/Nonlinear_regression)"],"rdfs:label":["Nonlinear Regression Learning"],"rdfs:seeAlso":["http://d3fend.mitre.org/ontologies/d3fend.owl#NonlinearRegression"],"rdfs:subClassOf":[{"@id":"d3f:RegressionAnalysisLearning"}]},{"@id":"d3f:NumericPatternMatching","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-NPM"],"d3f:definition":["Numeric pattern matching uses a pattern specification and sees if the numeric value matches that pattern--simple forms include exact matching and range matching."],"rdfs:hasSubClass":[{"@id":"d3f:ExactMatching"},{"@id":"d3f:RangeMatching"}],"rdfs:label":["Numeric Pattern Matching"],"rdfs:subClassOf":[{"@id":"d3f:PatternMatching"}]},{"@id":"d3f:OWL","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-OWL"],"d3f:definition":["The Web Ontology Language (OWL) is a family of knowledge representation languages for authoring ontologies."],"d3f:kb-article":["## How it works\nOntologies are a formal way to describe taxonomies and classification networks, essentially defining the structure of knowledge for various domains: the nouns representing classes of objects and the verbs representing relations between the objects.\n\nThe OWL languages are characterized by formal semantics. They are built upon the World Wide Web Consortium's (W3C) standard for objects called the Resource Description Framework (RDF). OWL classes correspond to description logic (DL) _concepts_.  OWL properties to DL _roles_, and individuals are named the same way in OWL and other DLs.\n\n## References\n1. Web Ontology Language. (2023, April 23). In _Wikipedia_. [Link](https://en.wikipedia.org/wiki/Web_Ontology_Language)"],"d3f:synonym":["Web Ontology Language"],"rdfs:label":["OWL"],"rdfs:subClassOf":[{"@id":"d3f:DescriptionLogic"}]},{"@id":"d3f:Parameter-basedTransferLearning","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-PBTL"],"d3f:definition":["The idea behind parameter-based methods is that a well-trained model on the source domain has learned a well-defined structure, and if two tasks are related, this structure can be transferred to the target model."],"d3f:kb-article":["## References\nGeorgian Impact Blog. (n.d.). Transfer Learning Part 1. [Link](https://medium.com/georgian-impact-blog/transfer-learning-part-1-ed0c174ad6e7#:~:text=Homogeneous%20Transfer%20Learning-,1.,the%20target%20domain%20for%20training)."],"rdfs:label":["Parameter-based Transfer Learning"],"rdfs:subClassOf":[{"@id":"d3f:HomogenousTransferLearning"}]},{"@id":"d3f:ParametricTests","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-PT"],"d3f:definition":["A parametric test relies upon the assumption that the data you want to test is (or approximately is) normally distributed."],"d3f:kb-article":["## References\nNewcastle University. (n.d.). Parametric Hypothesis Tests. [Link](https://www.ncl.ac.uk/webtemplate/ask-assets/external/maths-resources/psychology/parametric-hypothesis-tests.html)"],"rdfs:label":["Parametric Tests"],"rdfs:subClassOf":[{"@id":"d3f:HypothesisTesting"}]},{"@id":"d3f:PartialMatching","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-PM"],"d3f:definition":["Partial string pattern matching is a special case of string pattern matching where one seeks to find a pattern within a larger string (text). It allows for the detection of patterns that occur as substrings or partial segments within the full string, rather than requiring an exact match across the entire string."],"d3f:kb-article":["## References\n1. String-searching algorithm. (2023, April 8). In _Wikipedia_. [Link](https://en.wikipedia.org/wiki/String-searching_algorithm)","Numeric pattern matching is a method of matching some defined pattern specification against a numeric value."],"rdfs:hasSubClass":[{"@id":"d3f:RegexMatching"},{"@id":"d3f:SubstringMatching"},{"@id":"d3f:SoundexMatching"},{"@id":"d3f:ApproximateStringMatching"}],"rdfs:label":["Partial Matching"],"rdfs:subClassOf":[{"@id":"d3f:StringPatternMatching"}]},{"@id":"d3f:PatternMatching","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-PM"],"d3f:definition":["Pattern matching is the act of checking a given sequence of tokens for the presence of the constituents of some pattern."],"d3f:kb-article":["## References\n1. Pattern matching. (2023, May 20). In _Wikipedia_. [Link](https://en.wikipedia.org/wiki/Pattern_matching)"],"rdfs:hasSubClass":[{"@id":"d3f:StringPatternMatching"},{"@id":"d3f:NumericPatternMatching"}],"rdfs:label":["Pattern Matching"],"rdfs:subClassOf":[{"@id":"d3f:LogicalRules"}]},{"@id":"d3f:PearsonsCorrelationCoefficient","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-PCC"],"d3f:kb-article":["## References\nWolfram MathWorld. (n.d.). Correlation Coefficient. [Link](https://mathworld.wolfram.com/CorrelationCoefficient.html)"],"rdfs:label":["Pearson's Correlation Coefficient"],"rdfs:subClassOf":[{"@id":"d3f:Correlation"}]},{"@id":"d3f:Perturbation-basedLearning","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-PBL"],"d3f:definition":["Perturbation based methods are proposed under the smoothness assumption, which indicates that two data points close to each other in feature space are likely to have the same label."],"d3f:kb-article":["## References\nZheng, Y., & Song, Y. (2021). An Effective Perturbation-Based Semi-Supervised Learning Method for Acoustic Event Classification. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 29, 3580-3591. [Link](https://www.semanticscholar.org/paper/An-Effective-Perturbation-Based-Semi-Supervised-for-Zheng-Song/b75ae37d137ac354eb2ed42917e461b4dccdc977).\n\nEngelen, S., & Hoos, H. (2020). A survey on semi-supervised learning. Machine Learning, 109(2), 299-337. [Link](https://link.springer.com/article/10.1007/s10994-019-05855-6)."],"rdfs:label":["Perturbation-based Learning"],"rdfs:subClassOf":[{"@id":"d3f:IntrinsicallySemi-supervisedLearning"}]},{"@id":"d3f:PhiCoefficient","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-PC"],"d3f:definition":["The phi coefficient (or mean square contingency coefficient is a measure of association for two binary variables."],"d3f:kb-article":["## References\n\\Wikipedia. (n.d.). Phi coefficient. [Link](https://en.wikipedia.org/wiki/Phi_coefficient)"],"d3f:synonym":["MCC","Matthews Correlation Coefficient (in machine learning)"],"rdfs:label":["Phi Coefficient"],"rdfs:subClassOf":[{"@id":"d3f:Correlation"}]},{"@id":"d3f:Pix2Pix","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-PIX"],"d3f:definition":["Pix2Pix is based on condtional GAN architecture and are trained on paired set of images or scenes from two domains to be used for translation."],"d3f:kb-article":["## References\nEsri. (n.d.). How Pix2Pix Works. [Link](https://developers.arcgis.com/python/guide/how-pix2pix-works/)"],"rdfs:label":["Pix2Pix"],"rdfs:subClassOf":[{"@id":"d3f:Image-to-ImageTranslationGAN"}]},{"@id":"d3f:Point-biserialCorrelationCoefficient","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-PBCC"],"d3f:definition":["The point biserial correlation coefficient (rpb) is a correlation coefficient used when one variable (e.g. Y) is dichotomous; Y can either be \"naturally\" dichotomous, like whether a coin lands heads or tails, or an artificially dichotomized variable."],"d3f:kb-article":["## References\nWikipedia. (n.d.). Point-biserial correlation coefficient. [Link](https://en.wikipedia.org/wiki/Point-biserial_correlation_coefficient)"],"rdfs:label":["Point-biserial Correlation Coefficient"],"rdfs:subClassOf":[{"@id":"d3f:Correlation"}]},{"@id":"d3f:PointEstimation","@type":["owl:Class","owl:NamedIndividual"],"d3f:d3fend-id":["D3A-PE"],"d3f:definition":["A point estimation is a single value that estimates the parameter. Point estimates are single values calculated from the sample"],"d3f:kb-article":["## References\nPennsylvania State University. (n.d.). Statistical Inference and Estimation. [Link](https://online.stat.psu.edu/stat504/lesson/statistical-inference-and-estimation)"],"rdfs:label":["Point Estimation"],"rdfs:subClassOf":[{"@id":"d3f:Estimation"}]},{"@id":"d3f:PolicyGradient","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-PG"],"d3f:definition":["The objective of a Reinforcement Learning Policy Gradient agent is to maximize the “expected” reward when following a policy"],"d3f:kb-article":["## References\nPolicy Gradients in a Nutshell. Towards Data Science.  [Link](https://towardsdatascience.com/policy-gradients-in-a-nutshell-8b72f9743c5d)."],"rdfs:hasSubClass":[{"@id":"d3f:Actor-Critic"}],"rdfs:label":["Policy Gradient"],"rdfs:subClassOf":[{"@id":"d3f:Model-freeReinforcementLearning"}]},{"@id":"d3f:PredicateLogic","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-PL"],"d3f:definition":["Predicate logic is is collection of formal systems used in mathematics, philosophy, linguistics, and computer science. First-order logic and Higher-order logic both incorporate predicate logic."],"d3f:kb-article":["## References\n1. First-order logic. (2023, May 26). In _Wikipedia_. [Link](https://en.wikipedia.org/wiki/First-order_logic)\n2. Higher-order logic. (2023, May 13)\n[Link](https://en.wikipedia.org/wiki/Higher-order_logic)"],"rdfs:hasSubClass":[{"@id":"d3f:First-orderLogic"},{"@id":"d3f:Higher-orderLogic"}],"rdfs:label":["Predicate Logic"],"rdfs:subClassOf":[{"@id":"d3f:SymbolicAI"}]},{"@id":"d3f:PrincipalComponentAnalysis","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-PCA"],"d3f:definition":["Principal components analysis (PCA) creates a new set of orthogonal variables that contain the same information as the original set. It rotates the axes of variation to give a new set of orthogonal axes, ordered so that they summarize decreasing proportions of the variation."],"d3f:kb-article":["## References\nWikipedia. (n.d.). Multivariate statistics. [Link](https://en.wikipedia.org/wiki/Multivariate_statistics)"],"d3f:synonym":["PCA"],"rdfs:label":["Principal Component Analysis"],"rdfs:subClassOf":[{"@id":"d3f:MultivariateAnalysis"}]},{"@id":"d3f:PrincipalComponentsAnalysis","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-PCA"],"d3f:definition":["Principal Component Analysis (PCA) is a statistic-based method of identifying patterns in a large dataset while increasing interpretability and preserving information."],"d3f:kb-article":["## References\nWikipedia. (n.d.). Principal component analysis. [Link](https://en.wikipedia.org/wiki/Principal_component_analysis)"],"rdfs:label":["Principal Components Analysis"],"rdfs:subClassOf":[{"@id":"d3f:DimensionReduction"}]},{"@id":"d3f:ProbabilisticLogic","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-PL"],"d3f:definition":["Probabilistic logic extends traditional logic truth tables with probabilistic expressions."],"d3f:kb-article":["## References\n1. Probabilistic logic. (2023, June 5). In _Wikipedia_. [Link](https://en.wikipedia.org/wiki/Probabilistic_logic)"],"rdfs:label":["Probabilistic Logic"],"rdfs:subClassOf":[{"@id":"d3f:SymbolicAI"}]},{"@id":"d3f:ProgressivelyGrowingGAN","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-PGG"],"d3f:definition":["Progressive Growing GAN (ProGAN) is an extension to the GAN training process that allows for the stable training of generator models that can output large high-quality images."],"d3f:kb-article":["## References\n\nMachine Learning Mastery. (n.d.). Introduction to Progressive Growing Generative Adversarial Networks. [Link](https://machinelearningmastery.com/introduction-to-progressive-growing-generative-adversarial-networks/)"],"d3f:synonym":["ProGAN"],"rdfs:label":["Progressively Growing GAN"],"rdfs:subClassOf":[{"@id":"d3f:ImageSynthesisGAN"}]},{"@id":"d3f:ProjectedClustering","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-PC"],"d3f:definition":["Projected clustering is a dimension reduction subspace clustering method."],"d3f:kb-article":["## References\nGeeksforGeeks. (n.d.). Projected Clustering in Data Analytics. [Link](https://www.geeksforgeeks.org/projected-clustering-in-data-analytics/)"],"rdfs:label":["Projected Clustering"],"rdfs:subClassOf":[{"@id":"d3f:High-dimensionClustering"}]},{"@id":"d3f:Projection-basedClustering","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-PBC"],"d3f:definition":["Projection Based Clustering is a clustering framework based on a chosen projection method and projection method a parameter-free high-dimensional data visualization technique."],"d3f:kb-article":["## References\nR Core Team. (2021). ProjectionBasedClustering: Projection Based Clustering. [Link](https://cran.r-project.org/web/packages/ProjectionBasedClustering/ProjectionBasedClustering.pdf)\n\nLai, J. H., Liu, Y., & Wu, W. (2017). Projection Based Clustering. [Link](https://www.researchgate.net/publication/319006501_Projection_Based_Clustering)"],"rdfs:hasSubClass":[{"@id":"d3f:t-SNEClustering"}],"rdfs:label":["Projection-based Clustering"],"rdfs:subClassOf":[{"@id":"d3f:High-dimensionClustering"}]},{"@id":"d3f:Prolog","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-PRO"],"d3f:definition":["Prolog has its roots in first-order logic, a formal logic, and unlike many other programming languages."],"d3f:kb-article":["## How it works\nProlog is intended primarily as a declarative programming language: the program logic is expressed in terms of relations, represented as facts and rules. A computation is initiated by running a query over these relations.\n\n## References\n1. Prolog. (2023, April 5). In _Wikipedia_. [Link](https://en.wikipedia.org/wiki/Prolog)"],"rdfs:label":["Prolog"],"rdfs:subClassOf":[{"@id":"d3f:LogicProgramming"}]},{"@id":"d3f:PropositionalLogic","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-PL"],"d3f:definition":["Propositional logic deals with statements (i.e., propositions, which can be true or false) and relations between propositions, including the construction of arguments based on them."],"d3f:kb-article":["## How it works\nCompound propositions are formed by connecting propositions by logical connectives. Propositions that contain no logical connectives are called atomic propositions.\n\n## References\n1. Propositional Calculus. (2022, May 31). In _Wikipedia_. [Link](https://en.wikipedia.org/wiki/Propositional_calculus)"],"d3f:synonym":["Propositional Calculus"],"rdfs:label":["Propositional Logic"],"rdfs:subClassOf":[{"@id":"d3f:SymbolicAI"}]},{"@id":"d3f:Q-Learning","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-QL"],"d3f:definition":["Q-learning is a model-free reinforcement learning algorithm to learn the value of an action in a particular state. It does not require a model of the environment (hence \"model-free\"), and it can handle problems with stochastic transitions and rewards without requiring adaptations."],"d3f:kb-article":["## References\nQ-learning. Wikipedia. [Link](https://en.wikipedia.org/wiki/Q-learning)."],"rdfs:hasSubClass":[{"@id":"d3f:DeepQ-learning"}],"rdfs:label":["Q-Learning"],"rdfs:subClassOf":[{"@id":"d3f:Model-freeReinforcementLearning"}]},{"@id":"d3f:QueryByCommittee","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-QBC"],"d3f:definition":["Query by Committee (QBC) takes inspiration from ensemble methods. Instead of just one classifier, it takes into account the decision of a committee 𝐶=ℎ1,…,ℎc of classifiers ℎ𝑖. Each classifier has the same target classes, but a different underlying model or a different view on the data."],"d3f:kb-article":["## References\nIntro to Active Learning. inovex Blog.  [Link](https://www.inovex.de/de/blog/intro-to-active-learning/)."],"rdfs:label":["Query By Committee"],"rdfs:subClassOf":[{"@id":"d3f:ActiveLearning"}]},{"@id":"d3f:RandomForest","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-RF"],"d3f:definition":["Random Forest is a ML method that combines several other ML methods. At its core, Random Forest is an ensemble method of multiple bootstrapped decision trees filled with training data and random feature selection."],"d3f:kb-article":["## References\nRandom forest. Wikipedia.  [Link](https://en.wikipedia.org/wiki/Random_forest)."],"rdfs:label":["Random Forest"],"rdfs:subClassOf":[{"@id":"d3f:BootstrapAggregating"}]},{"@id":"d3f:RandomSplits","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-RS"],"d3f:definition":["The dataset is repeatedly sampled with a random split of the data into train and test sets."],"d3f:kb-article":["## References\nHow to Create a Random Split Cross-Validation and Bagging Ensemble for Deep Learning in Keras.\"*Machine Learning Mastery*.  [Link](https://machinelearningmastery.com/how-to-create-a-random-split-cross-validation-and-bagging-ensemble-for-deep-learning-in-keras/)."],"rdfs:label":["Random Splits"],"rdfs:subClassOf":[{"@id":"d3f:ResamplingEnsemble"}]},{"@id":"d3f:Range","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-RAN"],"d3f:definition":["The range of a set of data is the difference between the largest and smallest value."],"d3f:kb-article":["## References\nWikipedia. (n.d.). Range (statistics). [Link](https://en.wikipedia.org/wiki/Range_(statistics))"],"rdfs:label":["Range"],"rdfs:subClassOf":[{"@id":"d3f:Variability"}]},{"@id":"d3f:RangeMatching","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-RM"],"d3f:definition":["Numeric Range Matching determines if a value lies with an interval of values (i.e., within the range of values.)"],"rdfs:label":["Range Matching"],"rdfs:subClassOf":[{"@id":"d3f:NumericPatternMatching"}]},{"@id":"d3f:RankCorrelationCoefficient","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-RC"],"d3f:definition":["A rank correlation is any of several statistics that measure an ordinal association-the relationship between rankings of different ordinal variables or different rankings of the same variable, where a \"ranking\" is the assignment of the ordering labels \"first\", \"second\", \"third\", etc. to different observations of a particular variable."],"d3f:kb-article":["Wikipedia. (n.d.). Rank correlation. [Link](https://en.wikipedia.org/wiki/Rank_correlation)"],"rdfs:hasSubClass":[{"@id":"d3f:KendallsRankCorrelationCoefficient"},{"@id":"d3f:GoodmanAndKruskalsGamma"},{"@id":"d3f:SomersD"},{"@id":"d3f:SpearmansRankCorrelationCoefficient"}],"rdfs:label":["Rank Correlation"],"rdfs:subClassOf":[{"@id":"d3f:Correlation"}]},{"@id":"d3f:RecurrentNeuralNetwork","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-RNN"],"d3f:definition":["Recurrent Nerual Networks (RNN) are a class of artificial neural networks where connections between nodes can create a cycle, allowing output from some nodes to affect subsequent input to the same nodes. This allows it to exhibit temporal dynamic behavior."],"d3f:kb-article":["## References\nWikipedia. (2021, September 7). Recurrent Neural Network. [Link](https://en.wikipedia.org/wiki/Recurrent_neural_network)"],"rdfs:hasSubClass":[{"@id":"d3f:LongShort-termMemory"},{"@id":"d3f:GatedRecurrentUnit"}],"rdfs:label":["Recurrent Neural Network"],"rdfs:subClassOf":[{"@id":"d3f:DeepNeuralNetClassification"}]},{"@id":"d3f:RegexMatching","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-RM"],"d3f:definition":["Regular expression matching is type of partial string matching using a regular expression, which is a sequence of characters that specifies a match pattern in text."],"d3f:kb-article":["## How it works\n\nA regular expression (shortened as regex or regexp) is a sequence of characters that specifies a match pattern in text. Usually such patterns are used by string-searching algorithms for \"find\" or \"find and replace\" operations on strings, or for input validation.\n\n## Key Test Considerations\n\n- **External review of regular expressions**: Regular expressions used in rules should be reviewed by a independent developer SME.  Regex testing and visualization tools may be used to aid this review.  Back-tests for failure modes identified during the review shoud be developed.  Regular expressions are easy to get wrong and may appear to work on limited tests; small mistakes can lead to unintended misses and matches.]\n\n- **Processing Performance Review**: Review of resource-intensive rules may be necessary if system performance degraded.  Look for cases of “exponential backtracking”  Some regexes are computationally expensive.\n\n## References\n1. Regular expression. (2023, June 1). In _Wikipedia_. [Link](https://en.wikipedia.org/wiki/Regular_expression).\n2. String-searching algorithm. (2023, April 8). In _Wikipedia_. [Link](https://en.wikipedia.org/wiki/String-searching_algorithm)."],"d3f:synonym":["Regex","Regexp"],"rdfs:label":["Regex Matching"],"rdfs:subClassOf":[{"@id":"d3f:PartialMatching"}]},{"@id":"d3f:RegressionAnalysis","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-RA"],"d3f:definition":["Regression analysis is a set of statistical processes for estimating the relationships between a dependent variable (often called the 'outcome' or 'response' variable, or a 'label' in machine learning parlance) and one or more independent variables (often called 'predictors', 'covariates', 'explanatory variables' or 'features')."],"d3f:kb-article":["## References\nWikipedia. (n.d.). Regression analysis. [Link](https://en.wikipedia.org/wiki/Regression_analysis)"],"rdfs:hasSubClass":[{"@id":"d3f:LinearRegression"},{"@id":"d3f:BayesianLinearRegression"},{"@id":"d3f:NonlinearRegression"},{"@id":"d3f:LogisticRegression"},{"@id":"d3f:MultipleRegression"}],"rdfs:label":["Regression Analysis"],"rdfs:subClassOf":[{"@id":"d3f:StatisticalMethod"}]},{"@id":"d3f:RegressionAnalysisLearning","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-RAL"],"d3f:definition":["Regression is used to understand the relationship between dependent and independent variables which is then used to make projections, such as for sales revenue for a given business."],"d3f:kb-article":["## References\nSupervised Learning. IBM. [Link](https://www.ibm.com/topics/supervised-learning)."],"rdfs:hasSubClass":[{"@id":"d3f:DecisionTreeRegression"},{"@id":"d3f:BayesianLinearRegressionLearning"},{"@id":"d3f:LinearRegressionLearning"},{"@id":"d3f:LogististicRegressionLearning"},{"@id":"d3f:MultipleRegressionLearning"},{"@id":"d3f:NonlinearRegressionLearning"}],"rdfs:label":["Regression Analysis Learning"],"rdfs:subClassOf":[{"@id":"d3f:SupervisedLearning"}]},{"@id":"d3f:ReinforcementLearning","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-RL"],"d3f:definition":["Reinforcement Learning is a subjugate technique of machine learning that uses feedback to reinforce good or valid rules and lessen the reliance of bad or ineffective rules"],"d3f:kb-article":["## References\nReinforcement learning. Wikipedia.  [Link](https://en.wikipedia.org/wiki/Reinforcement_learning)."],"rdfs:hasSubClass":[{"@id":"d3f:Model-basedReinforcementLearning"},{"@id":"d3f:Model-freeReinforcementLearning"}],"rdfs:label":["Reinforcement Learning"],"rdfs:subClassOf":[{"@id":"d3f:MachineLearning"}]},{"@id":"d3f:Relational-basedTransferLearning","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-RBTL"],"d3f:definition":["Relational-based Transfer Learning is a subfield of machine learning where knowledge and patterns learned from one domain, characterized by relational and structured data, are transferred to enhance the learning of another related domain. This approach leverages shared concepts, relations, and structures across domains, taking advantage of the rich semantic knowledge within relational data to improve learning performance in the target task."],"d3f:kb-article":["## References\nV7 Labs. (n.d.). Transfer Learning Guide. [Link](https://www.v7labs.com/blog/transfer-learning-guide#:~:text=Relational%2Dbased%20transfer%20learning%20approaches,domain%20to%20the%20target%20domain)."],"rdfs:label":["Relational-based Transfer Learning"],"rdfs:subClassOf":[{"@id":"d3f:HomogenousTransferLearning"}]},{"@id":"d3f:ResamplingEnsemble","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-RE"],"d3f:definition":["In the method, the small classes are oversampled and large classes are undersampled. The resampling scale is determined by the ratio of the min class number and max class number. And multiple machine learning methods are selected to construct the ensemble"],"d3f:kb-article":["## References\nEnsemble learning. Wikipedia. [Link](https://en.wikipedia.org/wiki/Ensemble_learning).\n\nTorgo, L. (2014). A resampling ensemble algorithm for improved accuracy. *Neurocomputing*, 134, 55-66.  [Link](https://www.sciencedirect.com/science/article/pii/S0925231214007644#:~:text=A%20resampling%20ensemble%20algorithm%20is,and%20undersampling%20are%20empirically%20analyzed)."],"rdfs:hasSubClass":[{"@id":"d3f:BootstrapAggregating"},{"@id":"d3f:K-FoldCross-Validation"},{"@id":"d3f:RandomSplits"}],"rdfs:label":["Resampling Ensemble"],"rdfs:subClassOf":[{"@id":"d3f:EnsembleLearning"}]},{"@id":"d3f:ResidualNeuralNetwork","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-RNN"],"d3f:definition":["A residual neural network (ResNet) is an artificial neural network (ANN). It is a gateless or open-gated variant of the HighwayNet, the first working very deep feedforward neural network with hundreds of layers, much deeper than previous neural networks."],"d3f:kb-article":["## References\nWikipedia contributors. (2021, August 23). Residual neural network. In Wikipedia, The Free Encyclopedia. [Link](https://en.wikipedia.org/wiki/Residual_neural_network)"],"rdfs:label":["Residual Neural Network"],"rdfs:subClassOf":[{"@id":"d3f:ConvolutionalNeuralNetwork"}]},{"@id":"d3f:SARSA","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-SAR"],"d3f:definition":["State-action-reward-state-action (SARSA) is an algorithm for learning a Markov decision process policy, used in the reinforcement learning area of machine learning."],"d3f:kb-article":["## References\nState-action-reward-state-action. Wikipedia.  [Link](https://en.wikipedia.org/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action)."],"rdfs:isDefinedBy":["https://dbpedia.org/page/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action"],"rdfs:label":["SARSA"],"rdfs:subClassOf":[{"@id":"d3f:Model-freeReinforcementLearning"}]},{"@id":"d3f:Self-organizingMap","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-SOM"],"d3f:definition":["A Self-Organizing Map (SOM) is a unsupervised learning model in Artificial Neural Network where the feature maps are the generated two-dimensional discretized form of an input space during the model training (based on competitive learning)"],"d3f:kb-article":["## References\nGeeksforGeeks. (n.d.). ANN - Self Organizing Neural Network (SONN). [Link](https://www.geeksforgeeks.org/ann-self-organizing-neural-network-sonn/)"],"rdfs:label":["Self-organizing Map"],"rdfs:subClassOf":[{"@id":"d3f:ANN-basedClustering"}]},{"@id":"d3f:Semi-SupervisedLearning","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-SSL"],"d3f:definition":["Semi-supervised learning is a branch of machine learning that combines a small amount of labeled data with a large amount of unlabeled data during training."],"d3f:kb-article":["## References\nSemi-Supervised Learning. Wikipedia.  [Link](https://en.wikipedia.org/wiki/Semi-Supervised_Learning)."],"rdfs:hasSubClass":[{"@id":"d3f:Semi-supervisedInductiveLearning"},{"@id":"d3f:Semi-supervisedTransductiveLearning"},{"@id":"d3f:Semi-supervisedWrapperMethod"},{"@id":"d3f:UnsupervisedPreprocessing"},{"@id":"d3f:IntrinsicallySemi-supervisedLearning"}],"rdfs:label":["Semi-Supervised Learning"],"rdfs:seeAlso":["https://link.springer.com/article/10.1007/s10994-019-05855-6"],"rdfs:subClassOf":[{"@id":"d3f:MachineLearning"}]},{"@id":"d3f:Semi-supervisedBoosting","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-SSB"],"d3f:definition":["Boosting methods can be readily extended to the semi-supervised setting, by introducing pseudo-labeled data after each learning step; which gives rise to the idea of semi-supervised boosting methods. The pseudo-labeling approach of self- training and co-training can be easily extended to boosting methods. Several boosting methods such as SSMBoost, ASSEMBLE, SemiBoost, RegBoost, etc can be found which can be applied for utilizing unlabeled datasets for supervised classifiers."],"d3f:kb-article":["## References\nJashish Shrestha. (n.d.). Beginner's Guide to Semi-Supervised Learning. [Link](http://jashish.com.np/blog/posts/beginners-guide-to-semi-supervised-learning/)"],"rdfs:label":["Semi-supervised Boosting"],"rdfs:subClassOf":[{"@id":"d3f:Semi-supervisedWrapperMethod"}]},{"@id":"d3f:Semi-supervisedCluster-then-label","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-SSCTL"],"d3f:definition":["Pre-training methods are aimed to guide the parameters of a network towards interesting regions in model space using unlabeled data, before fine-tuning the parameters with the labeled data."],"d3f:kb-article":["## References\nJashish Shrestha. (n.d.). Beginner's Guide to Semi-Supervised Learning. [Link](http://jashish.com.np/blog/posts/beginners-guide-to-semi-supervised-learning/)"],"rdfs:label":["Semi-supervised Cluster-then-label"],"rdfs:subClassOf":[{"@id":"d3f:UnsupervisedPreprocessing"}]},{"@id":"d3f:Semi-supervisedCo-training","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-SSCT"],"d3f:definition":["Multi-view co-training involves training the classifiers in completely different views of training data. On the other hand, single-view co-training methods are generally applied as ensemble methods."],"d3f:kb-article":["## References\nJashish Shrestha. (n.d.). Beginner's Guide to Semi-Supervised Learning. [Link](http://jashish.com.np/blog/posts/beginners-guide-to-semi-supervised-learning/)"],"rdfs:label":["Semi-supervised Co-training"],"rdfs:subClassOf":[{"@id":"d3f:Semi-supervisedWrapperMethod"}]},{"@id":"d3f:Semi-supervisedFeatureExtraction","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-SSFE"],"d3f:definition":["Feature extraction refers to reducing the number of dimensions in a data point so that it is computationally feasible and effective to learn a model."],"d3f:kb-article":["## References\nJashish Shrestha. (n.d.). Beginner's Guide to Semi-Supervised Learning. [Link](http://jashish.com.np/blog/posts/beginners-guide-to-semi-supervised-learning/)"],"rdfs:label":["Semi-supervised Feature Extraction"],"rdfs:subClassOf":[{"@id":"d3f:UnsupervisedPreprocessing"}]},{"@id":"d3f:Semi-supervisedGenerativeModelLearning","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-SSGML"],"d3f:definition":["A Semi Supervised Machine Learning model which assume that the distributions take some particular form p(x|y,theta) parameterized by the vector. If these assumptions are incorrect, the unlabeled data may actually decrease the accuracy of the solution relative to what would have been obtained from labeled data alone. However, if the assumptions are correct, then the unlabeled data necessarily improves performance."],"d3f:kb-article":["## References\nWeak supervision. Wikipedia.  [Link](https://en.wikipedia.org/wiki/Weak_supervision#Generative_models)."],"rdfs:label":["Semi-supervised Generative Model Learning"],"rdfs:subClassOf":[{"@id":"d3f:IntrinsicallySemi-supervisedLearning"}]},{"@id":"d3f:Semi-supervisedInductiveLearning","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-SSIL"],"d3f:definition":["The goal of inductive learning is to infer the correct mapping from\nX to Y"],"d3f:kb-article":["## References\nSemi-Supervised Learning. Wikipedia.  [Link](https://en.wikipedia.org/wiki/Semi-Supervised_Learning#Semi-supervised_learning).\n\nZhou, D., & Li, M. (2005). Semi-supervised learning by higher order regularization. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL) (pp. 1-9).  [Link](https://www.cs.sfu.ca/~anoop/papers/pdf/semisup_naacl.pdf)."],"rdfs:label":["Semi-supervised Inductive Learning"],"rdfs:subClassOf":[{"@id":"d3f:Semi-SupervisedLearning"}]},{"@id":"d3f:Semi-supervisedManifoldLearning","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-SSML"],"d3f:definition":["A version of Semi-Supervised Learning that applies the Manifold assumption that the data like approximately on a manifold of much lower dimension than the input space."],"d3f:kb-article":["## References\nWeak supervision. Wikipedia.  [Link](https://en.wikipedia.org/wiki/Weak_supervision#Generative_models)."],"rdfs:label":["Semi-supervised Manifold Learning"],"rdfs:subClassOf":[{"@id":"d3f:IntrinsicallySemi-supervisedLearning"}]},{"@id":"d3f:Semi-supervisedPre-training","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-SSPT"],"d3f:definition":["Pre-training methods are aimed to guide the parameters of a network towards interesting regions in model space using unlabeled data, before fine-tuning the parameters with the labeled data"],"d3f:kb-article":["## References\nJashish Shrestha. (n.d.). Beginner's Guide to Semi-Supervised Learning. [Link](http://jashish.com.np/blog/posts/beginners-guide-to-semi-supervised-learning/)"],"rdfs:label":["Semi-supervised Pre-training"],"rdfs:subClassOf":[{"@id":"d3f:UnsupervisedPreprocessing"}]},{"@id":"d3f:Semi-supervisedSelf-training","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-SSST"],"d3f:definition":["Self-training is the procedure in which a supervised method for classification or regression is modified it to work in a semi-supervised manner, taking advantage of labeled and unlabeled data"],"d3f:kb-article":["## References\nAltexSoft. (n.d.). Semi-Supervised Learning: A Technical Guide with Python Examples. [Link](https://www.altexsoft.com/blog/semi-supervised-learning/#:~:text=One%20of%20the%20simplest%20examples,of%20labeled%20and%20unlabeled%20data.)"],"rdfs:label":["Semi-supervised Self-training"],"rdfs:subClassOf":[{"@id":"d3f:Semi-supervisedWrapperMethod"}]},{"@id":"d3f:Semi-supervisedTransductiveLearning","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-SSTL"],"d3f:definition":["The goal of transductive learning is to infer the correct labels for the given unlabeled data\nx_{l+1},... ,x_{l+u} only"],"d3f:kb-article":["## References\nSemi-Supervised Learning. Wikipedia.  [Link](https://en.wikipedia.org/wiki/Semi-Supervised_Learning#Semi-supervised_learning).\n\nZhou, D., & Li, M. (2005). Semi-supervised learning by higher order regularization. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL) (pp. 1-9). [Link](https://www.cs.sfu.ca/~anoop/papers/pdf/semisup_naacl.pdf)."],"rdfs:hasSubClass":[{"@id":"d3f:Graph-basedSemi-supervisedLearning"}],"rdfs:label":["Semi-supervised Transductive Learning"],"rdfs:subClassOf":[{"@id":"d3f:Semi-SupervisedLearning"}]},{"@id":"d3f:Semi-supervisedWrapperMethod","@type":["owl:Class","owl:NamedIndividual"],"d3f:d3fend-id":["D3A-SSWM"],"d3f:definition":["The principle behind wrapper methods is that we train a model with labeled data and then generate pseudo-labels for the unlabeled data using the trained model iteratively."],"d3f:kb-article":["## References\nJashish, M. (n.d.). Beginner's Guide to Semi-Supervised Learning. Jashish Blog.  [Link](http://jashish.com.np/blog/posts/beginners-guide-to-semi-supervised-learning/)."],"rdfs:hasSubClass":[{"@id":"d3f:Semi-supervisedBoosting"},{"@id":"d3f:Semi-supervisedCo-training"},{"@id":"d3f:Semi-supervisedSelf-training"}],"rdfs:label":["Semi-supervised Wrapper Method"],"rdfs:subClassOf":[{"@id":"d3f:Semi-SupervisedLearning"}]},{"@id":"d3f:SeqGAN","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-SEQ"],"d3f:definition":["Sequence Generation Framework (SeqGAN) models the data generator as a stochastic policy in reinforcement learning (RL), SeqGAN bypasses the generator differentiation problem by directly performing gradient policy update."],"d3f:kb-article":["## References\nYu, L., Zhang, W., Wang, J., & Yu, Y. (2017). SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient. ArXiv preprint ArXiv:1609.05473. [Link](https://arxiv.org/abs/1609.05473)"],"d3f:synonym":["Sequence GAN"],"rdfs:label":["SeqGAN"],"rdfs:subClassOf":[{"@id":"d3f:GenerativeAdversarialNetwork"}]},{"@id":"d3f:SingularValueDecomposition","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-SVD"],"d3f:definition":["Singular Value Decomposition (SVD) is an algorithm that represents a matrix as a linear series of data and to find the set of factors that will best predict an outcome"],"d3f:kb-article":["## References\nWikipedia. (n.d.). Singular value decomposition. [Link](https://en.wikipedia.org/wiki/Singular_value_decomposition)"],"rdfs:label":["Singular Value Decomposition"],"rdfs:subClassOf":[{"@id":"d3f:DimensionReduction"}]},{"@id":"d3f:Skewness","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-SKE"],"d3f:definition":["Skewness is a measure of the asymmetry of the probability distribution of a real-valued random variable about its mean. The standardized moment of a probability distribution function."],"d3f:kb-article":["## References\nWikipedia. (n.d.). Skewness. [Link](https://en.wikipedia.org/wiki/Skewness)"],"rdfs:label":["Skewness"],"rdfs:subClassOf":[{"@id":"d3f:DistributionProperties"}]},{"@id":"d3f:SomersD","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-SD"],"rdfs:label":["Somers' D"],"rdfs:subClassOf":[{"@id":"d3f:RankCorrelationCoefficient"}]},{"@id":"d3f:SoundexMatching","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-SM"],"d3f:definition":["Soundex is a phonetic algorithm for indexing names by sound, as pronounced in English."],"d3f:kb-article":["## How it works\nThe goal is for homophones to be encoded to the same representation so that they can be matched despite minor differences in spelling. The algorithm mainly encodes consonants; a vowel will not be encoded unless it is the first letter. Soundex is the most widely known of all phonetic algorithms (in part because it is a standard feature of popular database software. Improvements to Soundex are the basis for many modern phonetic algorithms.\n\n## References\n1. Soundex. (2023, April 19). In _Wikipedia_. [Link](https://en.wikipedia.org/wiki/Soundex)"],"rdfs:label":["Soundex Matching"],"rdfs:subClassOf":[{"@id":"d3f:PartialMatching"}]},{"@id":"d3f:SpearmansRankCorrelationCoefficient","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-SRCC"],"d3f:synonym":["Spearman's Rho"],"rdfs:label":["Spearman's Rank Correlation Coefficient"],"rdfs:subClassOf":[{"@id":"d3f:RankCorrelationCoefficient"}]},{"@id":"d3f:SpectralClustering","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-SC"],"d3f:definition":["Spectral clustering is a technique that identifies communities of nodes in a graph based on the edges connecting them."],"d3f:kb-article":["## References\nTowards Data Science. (n.d.). Spectral Clustering. [Link](https://towardsdatascience.com/spectral-clustering-aba2640c0d5b)"],"rdfs:label":["Spectral Clustering"],"rdfs:subClassOf":[{"@id":"d3f:Graph-basedClustering"}]},{"@id":"d3f:Stacking","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-STA"],"d3f:definition":["Stacking is a method of using the results and predictions from one layer of ML models as inputs to another layer of ML models. Stacking (sometimes called stacked generalization) involves training a model to combine the predictions of several other learning algorithms."],"d3f:kb-article":["## References\nEnsemble learning. Wikipedia.  [Link](https://en.wikipedia.org/wiki/Ensemble_learning)."],"rdfs:label":["Stacking"],"rdfs:subClassOf":[{"@id":"d3f:EnsembleLearning"}]},{"@id":"d3f:StandardDeviation","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-SD"],"d3f:definition":["The standard deviation is a measure of the amount of variation or dispersion of a set of values."],"d3f:kb-article":["## References\nWikipedia. (n.d.). Standard deviation. [Link](https://en.wikipedia.org/wiki/Standard_deviation)"],"d3f:synonym":["SD"],"rdfs:label":["Standard Deviation"],"rdfs:subClassOf":[{"@id":"d3f:Variability"}]},{"@id":"d3f:StatisticalMethod","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-SM"],"d3f:definition":["Building methods using the mathematical study of the likelihood and probability of events occurring based on known information and inferred by taking a limited number of samples."],"d3f:kb-article":["## References\nWolfram MathWorld. (n.d.). Statistics. [Link](https://mathworld.wolfram.com/Statistics.html)"],"rdfs:hasSubClass":[{"@id":"d3f:RegressionAnalysis"},{"@id":"d3f:DescriptiveStatistics"},{"@id":"d3f:MultivariateAnalysis"},{"@id":"d3f:BayesianMethod"},{"@id":"d3f:InferentialStatistics"},{"@id":"d3f:TimeSeriesAnalysis"}],"rdfs:label":["Statistical Method"],"rdfs:subClassOf":[{"@id":"d3f:AnalyticTechnique"}]},{"@id":"d3f:StringEquivalenceMatching","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-SEM"],"d3f:definition":["String equivalence matching is a type of string pattern matching which is exact; that is, the strings being compared must have the same value for each character in their sequence and be of the same length."],"d3f:kb-article":["## References\n1. String-searching algorithm. (2023, April 8). In _Wikipedia_. [Link](https://en.wikipedia.org/wiki/String-searching_algorithm)\n2. Types of Equality. (2007, March 2). In _WikiWikiWeb_. [Link](https://wiki.c2.com/?TypesOfEquality)"],"rdfs:label":["String Equivalence Matching"],"rdfs:subClassOf":[{"@id":"d3f:EquivalenceMatching"},{"@id":"d3f:StringPatternMatching"}]},{"@id":"d3f:StringPatternMatching","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-SPM"],"d3f:definition":["String pattern-matching algorithms, also known as string-matching algorithms, are an important class of string algorithms that try to find a place where one or several strings (also called patterns) are found within a larger string or text"],"d3f:kb-article":["## How it works\nA basic example of string searching is when the pattern and the searched text are arrays of elements of an alphabet (finite set) Σ. Σ may be a human language alphabet, for example, the letters A through Z and other applications may use a binary alphabet (Σ = {0,1}) or a DNA alphabet (Σ = {A,C,G,T}) in bioinformatics.\n\nIn practice, the method of feasible string-search algorithm may be affected by the string encoding. In particular, if a variable-width encoding is in use, then it may be slower to find the Nth character, perhaps requiring time proportional to N. This may significantly slow some search algorithms. One of many possible solutions is to search for the sequence of code units instead, but doing so may produce false matches unless the encoding is specifically designed to avoid it.\n\n## References\n1. String-searching algorithm. (2023, April 8). In _Wikipedia_. [Link](https://en.wikipedia.org/wiki/String-searching_algorithm)"],"rdfs:hasSubClass":[{"@id":"d3f:PartialMatching"},{"@id":"d3f:StringEquivalenceMatching"}],"rdfs:label":["String Pattern Matching"],"rdfs:subClassOf":[{"@id":"d3f:PatternMatching"}]},{"@id":"d3f:StyleGAN","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-STY"],"d3f:definition":["Successor to the ProGAN."],"d3f:kb-article":["## References\nWikipedia. (n.d.). StyleGAN. [Link](https://en.wikipedia.org/wiki/StyleGAN)"],"d3f:synonym":["Style GAN"],"rdfs:label":["StyleGAN"],"rdfs:subClassOf":[{"@id":"d3f:ImageSynthesisGAN"}]},{"@id":"d3f:SubspaceClustering","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-SC"],"d3f:definition":["Subspace clustering is an extension of traditional clustering that seeks to find clusters in different subspaces within a dataset."],"d3f:kb-article":["## References\nParsons, L., Haque, E., & Liu, H. (2004). Subspace Clustering for High Dimensional Data: A Review. [Link](https://www.kdd.org/exploration_files/parsons.pdf)"],"rdfs:label":["Subspace Clustering"],"rdfs:subClassOf":[{"@id":"d3f:CorrelationClustering"}]},{"@id":"d3f:SubstringMatching","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-SM"],"d3f:definition":["String-searching algorithms, sometimes called string-matching algorithms, are an important class of string algorithms that try to find a place where one or several strings (also called patterns) are found within a larger string or text."],"d3f:kb-article":["## References\n1. String-searching algorithm. (2023, April 8). In _Wikipedia_. [Link](https://en.wikipedia.org/wiki/String-searching_algorithm)"],"rdfs:label":["Substring Matching"],"rdfs:subClassOf":[{"@id":"d3f:PartialMatching"}]},{"@id":"d3f:SupervisedLearning","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-SL"],"d3f:definition":["Supervised learning establishes a relationship between the known input and output variables to conduct a predictive analysis."],"d3f:kb-article":["## References\nSupervised learning. Wikipedia.  [Link](https://en.wikipedia.org/wiki/Supervised_learning)."],"rdfs:hasSubClass":[{"@id":"d3f:Classification"},{"@id":"d3f:RegressionAnalysisLearning"}],"rdfs:label":["Supervised Learning"],"rdfs:subClassOf":[{"@id":"d3f:MachineLearning"}]},{"@id":"d3f:SupportVectorMachineClassification","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-SVMC"],"d3f:definition":["Support Vector Machine (SVM) is a robust classification and regression technique that maximizes the predictive accuracy of a model without overfitting the training data. SVM is particularly suited to analyzing data with very large numbers (for example, thousands) of predictor fields."],"d3f:kb-article":["## References\nAbout Support Vector Machine (SVM). IBM SPSS Modeler SaaS Documentation. [Link](https://www.ibm.com/docs/en/spss-modeler/saas?topic=models-about-svm&mhsrc=ibmsearch_a&mhq=support%20vector%20machine)."],"rdfs:label":["Support Vector Machine Classification"],"rdfs:subClassOf":[{"@id":"d3f:Classification"}]},{"@id":"d3f:SymbolicAI","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-SR"],"d3f:definition":["Symbolic artificial intelligence is the term for the collection of all methods in artificial intelligence that are based on high-level symbolic (human-readable) representations of problems, logic, and search."],"d3f:kb-article":["## How it works\nSymbolic artificial intelligence is used in tools such as logic programming, production rules, semantic nets and frames, and it developed applications such as knowledge-based systems (in particular, expert systems), symbolic mathematics, automated theorem provers, ontologies, the semantic web, and automated planning and scheduling systems. The Symbolic AI paradigm led to seminal ideas in search, symbolic programming languages, agents, multi-agent systems, the semantic web, and the strengths and limitations of formal knowledge and reasoning systems.\n\n## References\n1. Symbolic artifical intelligence. (2023, May 23). In _Wikipedia_. [Link](https://en.wikipedia.org/wiki/Symbolic_artificial_intelligence)"],"d3f:synonym":["Symbolic Artificial Intelligence"],"rdfs:hasSubClass":[{"@id":"d3f:LogicProgramming"},{"@id":"d3f:ModalLogic"},{"@id":"d3f:Non-monotonicLogic"},{"@id":"d3f:ProbabilisticLogic"},{"@id":"d3f:PropositionalLogic"},{"@id":"d3f:FuzzyLogic"},{"@id":"d3f:DescriptionLogic"},{"@id":"d3f:PredicateLogic"}],"rdfs:label":["Symbolic AI"],"rdfs:subClassOf":[{"@id":"d3f:SymbolicLogic"}]},{"@id":"d3f:SymbolicLogic","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-SL"],"d3f:definition":["Symbolic Logic, also known as formal logic, is a branch of mathematics that uses symbolic representations for logical expressions and relationships. It provides a systematic method for examining the structure of arguments and reasoning, focusing on the relationships between propositions rather than the content of those propositions."],"d3f:kb-article":["## How it Works\n\n## References\n1. Symbolic Logic. (2023, June 6). In _Wolfram Mathworld_. [Link](https://mathworld.wolfram.com/SymbolicLogic.html)\n2. Hughes, G. and Schagrin, M. (2023, Apr 19). Formal Logic. _Encyclopedia Brittanica_. [Link](https://www.britannica.com/topic/formal-logic)\n3. Carnap, R. (1953). Introduction to Symbolic Logic and Its Applications. Dover Publications. [Link](https://archive.org/details/rudolf-carnap-introduction-to-symbolic-logic-and-its-applications/page/3/mode/2up)"],"rdfs:hasSubClass":[{"@id":"d3f:LogicalRules"},{"@id":"d3f:SymbolicAI"}],"rdfs:label":["Symbolic Logic"],"rdfs:subClassOf":[{"@id":"d3f:AnalyticTechnique"}]},{"@id":"d3f:SymmetricFeature-basedTransferLearning","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-SFTL"],"d3f:definition":["Homogeneous symmetric transformation takes both the source feature space Xs and target feature space Xt and learns feature transformations as to project each onto a common subspace Xc for adaptation purposes. This derived subspace becomes a domain-invariant feature subspace to associate cross-domain data, and in effect, reduces marginal distribution differences."],"d3f:kb-article":["## References\nDay, O., & Khoshgoftaar, T.M. (2017). A survey on heterogeneous transfer learning. *Journal of Big Data, 4*(1), 29. [Link](https://doi.org/10.1186/s40537-017-0089-0)."],"rdfs:label":["Symmetric Feature-based Transfer Learning"],"rdfs:subClassOf":[{"@id":"d3f:HomogenousTransferLearning"}]},{"@id":"d3f:TemporalDifferenceLearning","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-TDL"],"d3f:definition":["Temporal difference (TD) learning refers to a class of model-free reinforcement learning methods which learn by bootstrapping from the current estimate of the value function. These methods sample from the environment, like Monte Carlo methods, and perform updates based on current estimates, like dynamic programming methods"],"d3f:kb-article":["## References\nTemporal difference learning. Wikipedia.  [Link](https://en.wikipedia.org/wiki/Temporal_difference_learning)."],"rdfs:comment":["Temporal difference (TD) learning refers to a class of model-free reinforcement learning methods which learn by bootstrapping from the current estimate of the value function."],"rdfs:hasSubClass":[{"@id":"d3f:Actor-Critic"}],"rdfs:label":["Temporal Difference Learning"],"rdfs:subClassOf":[{"@id":"d3f:Model-freeReinforcementLearning"}]},{"@id":"d3f:TemporalLogic","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-TL"],"d3f:definition":["Temporal logic addresses the semantics of tense; i.e., qualifying expressions of when."],"d3f:kb-article":["## References\n1. Temporal logic. (2023, June 4). In _Wikipedia_. [Link](https://en.wikipedia.org/wiki/Modal_logic#Temporal_logic)"],"rdfs:label":["Temporal Logic"],"rdfs:subClassOf":[{"@id":"d3f:ModalLogic"}]},{"@id":"d3f:TimeSeriesAnalysis","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-TSA"],"d3f:definition":["Time series analysis comprises methods for analyzing time series data in order to extract meaningful statistics and other characteristics of the data."],"d3f:kb-article":["## References\nWikipedia. (n.d.). Time series. [Link](https://en.wikipedia.org/wiki/Time_series)"],"rdfs:hasSubClass":[{"@id":"d3f:ARIMAModel"},{"@id":"d3f:AutoregressiveModel"},{"@id":"d3f:MovingAverageModel"},{"@id":"d3f:ARMA_Model"}],"rdfs:label":["Time Series Analysis"],"rdfs:subClassOf":[{"@id":"d3f:StatisticalMethod"}]},{"@id":"d3f:TransferLearning","@type":["owl:Class","owl:NamedIndividual"],"d3f:d3fend-id":["D3A-TL"],"d3f:definition":["Transfer learning (TL) is a research problem in machine learning (ML) that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem."],"d3f:kb-article":["## References\nTransfer learning. Wikipedia.  [Link](https://en.wikipedia.org/wiki/Transfer_learning)."],"rdfs:hasSubClass":[{"@id":"d3f:HeterogeneousTransferLearning"},{"@id":"d3f:HomogenousTransferLearning"}],"rdfs:label":["Transfer Learning"],"rdfs:seeAlso":["https://arxiv.org/abs/1808.01974","https://arxiv.org/abs/1911.02685","https://journalofbigdata.springeropen.com/articles/10.1186/s40537-016-0043-6"],"rdfs:subClassOf":[{"@id":"d3f:MachineLearning"}]},{"@id":"d3f:Transformer-XL","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-TX"],"d3f:definition":["Transformer-XL is a transformer architecture that introduces the notion of recurrence to the deep self-attention network. Instead of computing the hidden states from scratch for each new segment, Transformer-XL reuses the hidden states obtained in previous segments."],"d3f:kb-article":["## References\nTransformer-XL. (n.d.). Papers with Code. [Link](https://paperswithcode.com/method/transformer-xl)"],"rdfs:label":["Transformer-XL"],"rdfs:subClassOf":[{"@id":"d3f:Transformer-basedLearning"}]},{"@id":"d3f:Transformer-basedLearning","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-TBL"],"d3f:definition":["A transformer is a deep learning model. It is distinguished by its adoption of self-attention, differentially weighting the significance of each part of the input (which includes the recursive output) data."],"d3f:kb-article":["## References\n\"Transformer (machine learning model).\" Wikipedia. [Link](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))."],"rdfs:hasSubClass":[{"@id":"d3f:BERT"},{"@id":"d3f:GPT"},{"@id":"d3f:Transformer-XL"}],"rdfs:label":["Transformer-based Learning"],"rdfs:subClassOf":[{"@id":"d3f:MachineLearning"}]},{"@id":"d3f:TrimmedMean","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-TM"],"d3f:definition":["The arithmetic mean of data values after a certain number or proportion of the highest and lowest data values have been discarded."],"d3f:kb-article":["## References\nWikipedia. (n.d.). Central tendency. [Link](https://en.wikipedia.org/wiki/Central_tendency)"],"d3f:synonym":["Truncated mean"],"rdfs:label":["Trimmed Mean"],"rdfs:subClassOf":[{"@id":"d3f:CentralTendency"}]},{"@id":"d3f:UncertaintySampling","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-US"],"d3f:definition":["Makes the utility inversely proportional to the uncertainty of the model with respect to the sample and will work with any  model provided it can assess its uncertainty of a predection."],"d3f:kb-article":["## References\nIntro to Active Learning. inovex Blog.  [Link](https://www.inovex.de/de/blog/intro-to-active-learning/)."],"rdfs:label":["Uncertainty Sampling"],"rdfs:subClassOf":[{"@id":"d3f:ActiveLearning"}]},{"@id":"d3f:UnsupervisedLearning","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-UL"],"d3f:definition":["Unsupervised learning creates relationships with unlabeled data without the input of a human or other outside actor. Uses only input data. "],"d3f:kb-abstract":["## References\nUnsupervised learning. Wikipedia.  [Link](https://en.wikipedia.org/wiki/Unsupervised_learning)."],"rdfs:hasSubClass":[{"@id":"d3f:DimensionReduction"},{"@id":"d3f:AssociationRuleLearning"},{"@id":"d3f:ClusterAnalysis"},{"@id":"d3f:GenerativeAdversarialNetwork"}],"rdfs:label":["Unsupervised Learning"],"rdfs:subClassOf":[{"@id":"d3f:MachineLearning"}]},{"@id":"d3f:UnsupervisedPreprocessing","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-UP"],"d3f:definition":["When performing unsupervised learning, the machine is presented with unlabeled data. (Unlabeled data has no target.) Unsupervised learning algorithms seek to discover intrinsic patterns that underlie the data, such as a clustering parameter or a redundant parameter (dimension) that can be reduced."],"d3f:kb-article":["## References\nSAS Institute Inc. (n.d.). Decision Trees. In SAS® Visual Data Mining and Machine Learning.[Link](https://documentation.sas.com/doc/en/vdmmlcdc/8.4/vdmmladvug/n1e4spzcnv1f0fn1vsxhbzgdp1bb.htm)."],"rdfs:hasSubClass":[{"@id":"d3f:Semi-supervisedCluster-then-label"},{"@id":"d3f:Semi-supervisedFeatureExtraction"},{"@id":"d3f:Semi-supervisedPre-training"}],"rdfs:label":["Unsupervised Preprocessing"],"rdfs:subClassOf":[{"@id":"d3f:Semi-SupervisedLearning"}]},{"@id":"d3f:Variability","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-VAR"],"d3f:definition":["Dispersion (also called variability, scatter, or spread) is the extent to which a distribution is stretched or squeezed. A measure of statistical dispersion is a nonnegative real number that is zero if all the data are the same and increases as the data become more diverse."],"d3f:kb-article":["## References\nWikipedia. (n.d.). Statistical dispersion. [Link](https://en.wikipedia.org/wiki/Statistical_dispersion)"],"rdfs:hasSubClass":[{"@id":"d3f:StandardDeviation"},{"@id":"d3f:Variance"},{"@id":"d3f:AverageAbsoluteDeviation"},{"@id":"d3f:CoefficientOfVariation"},{"@id":"d3f:InterquartileRange"},{"@id":"d3f:Range"}],"rdfs:label":["Variability"],"rdfs:subClassOf":[{"@id":"d3f:DescriptiveStatistics"}]},{"@id":"d3f:Variance","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-VAR"],"d3f:definition":["Variance is a measure of dispersion, meaning it is a measure of how far a set of numbers is spread out from their average value."],"d3f:kb-article":["## References\nWikipedia. (n.d.). Variance. [Link](https://en.wikipedia.org/wiki/Variance)"],"rdfs:label":["Variance"],"rdfs:subClassOf":[{"@id":"d3f:Variability"}]},{"@id":"d3f:VarianceReduction","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-VR"],"d3f:definition":["Leverages a well-known result from statistical learning and decomposes the model error into a data noise term, a model bias term and a model variance term. As the noise term only depends on the data and the bias is induced by the choice of model, any reduction in the error can only come from the variance term."],"d3f:kb-article":["## References\nIntro to Active Learning. inovex Blog.  [Link](https://www.inovex.de/de/blog/intro-to-active-learning/)."],"rdfs:label":["Variance Reduction"],"rdfs:subClassOf":[{"@id":"d3f:ActiveLearning"}]},{"@id":"d3f:Voting","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-VOT"],"d3f:definition":["Voting is another form of ensembling."],"d3f:kb-article":["## References\nEnsemble learning. Wikipedia.  [Link](https://en.wikipedia.org/wiki/Ensemble_learning)."],"rdfs:label":["Voting"],"rdfs:subClassOf":[{"@id":"d3f:EnsembleLearning"}]},{"@id":"d3f:WeightedMean","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-WM"],"d3f:definition":["A mean that incorporates weighting to certain data elements."],"d3f:kb-article":["## Considerations\nThe arithmetic mean, geometric mean, and harmonic mean can all be weighted.\n\n## References\nWikipedia. (n.d.). Central tendency. [Link](https://en.wikipedia.org/wiki/Central_tendency)"],"rdfs:label":["Weighted Mean"],"rdfs:subClassOf":[{"@id":"d3f:CentralTendency"}]},{"@id":"d3f:t-SNEClustering","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":["D3A-TSC"],"d3f:definition":["T-distributed Stochastic Neighbor Embedding (t-SNE) is a statistical method for visualizing high-dimensional data by giving each datapoint a location in a two or three-dimensional map."],"d3f:kb-article":["## References\nWikipedia. (n.d.). T-distributed stochastic neighbor embedding. [Link](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding)"],"rdfs:label":["t-SNE Clustering"],"rdfs:subClassOf":[{"@id":"d3f:Projection-basedClustering"}]}]}