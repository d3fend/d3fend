{"def_to_off":{"head":{"vars":["query_def_tech_label","top_def_tech_label","def_tactic_label","def_tactic_rel_label","def_tech_label","def_artifact_rel_label","def_artifact_label","off_artifact_label","off_artifact_rel_label","off_tech_label","off_tech_id","off_tech_parent_label","off_tech_parent_is_toplevel","off_tactic_rel_label","off_tactic_label","def_tactic","def_tactic_rel","tactic_def_tech","def_tech","def_artifact_rel","def_artifact","off_artifact","off_artifact_rel","off_tech","off_tech_parent","off_tactic_rel","off_tactic"]},"results":{"bindings":[]}},"description":{"@context":{"rdfs":"http://www.w3.org/2000/01/rdf-schema#","owl":"http://www.w3.org/2002/07/owl#","d3f":"http://d3fend.mitre.org/ontologies/d3fend.owl#","skos":"http://www.w3.org/2004/02/skos/core#"},"@graph":[{"@id":"d3f:DecisionTree","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":"D3A-DT","d3f:definition":"Decision tree learning is a supervised learning approach used in statistics, data mining, and machine learning. In this formalism, a classification or regression decision tree is used as a predictive model to draw conclusions about a set of observations.","d3f:kb-article":"## How it works\n\nA decision tree starts with a root node, which does not have any incoming branches. The outgoing branches from the root node then feed into the internal nodes, also known as decision nodes. Based on the available features, both node types conduct evaluations to form homogenous subsets, which are denoted by leaf nodes, or terminal nodes. The leaf nodes represent all the possible outcomes within the dataset.\n\n## Considerations\n\nWhile the basic underlying model is that of a decision tree, the decision tree node criteria, and the method for identifying splits varies significantly depending on the learning algorithm selected (e.g., CART, ID3, C4.5, C5.0, CHAID, MARS.)  Extensions like linear and logistic trees can add additional expressiveness as well.\n\n## Key Test Considerations\n\n- **Machine Learning**:\n\n  - **Verify the dataset quality**: Check the data to make sure it is\n      free of errors.  Quantify the degree of missing values,\n      outliers, and noise in the data collection.  If the data quality\n      is low, it may be difficult or impossible to create models and\n      systems with the desired performance.\n\n  - **Verify development datasets are representative**: of expected\n      operational environment and data collection means.  Compare\n      distributions of dataset features and labels with exploratory\n      data analysis and assess the difference in tests on training\n      data and tests on evaluation data (where the evaluation data\n      must be drawn from a representative dataset.)\n\n   - **Use a variety of data sets**: where available and applicable, to\n      reflect different operating and environment conditions that are\n      likley to be be encountered.\n\n  - **Use software libraries**: and tools built for ML where possible, so\n      that the underlying code is verified by prior use.**\n\n  - **Diagnose model errors with domain SMEs**: Have problem domain\n    SMEs investigate model errors for conditions for which the model\n    may underperform and suggest refinements.\n\n- **Classification**:\n\n  - **Use Standard Classification Performance Measures**: Not all of\n      the following may be necessary, but should be considered for\n      both verification (developmental test) and operational test\n      stages use:\n\n    - **Accuracy**: The fraction of predictions that were corret.\n\n    - **Precision**: The proportion of positive identifications that were correct.\n\n    - **Recall**: The proportion of actual positive cases identified correctly.\n\n    - **F-Measure**: Combines the preicion and recall into a single\n        score.  It is the harmonic mean of the precision and recall.\n\n    - **Receiver Operating Characteristic (ROC) Curve**: A ROC curve\n        shows the performance of a classification model at all\n        classification thresholds.  It graphs the True Positive Rate\n        over the False Positive Rate.\n\n    - **Area Under the ROC Curve (AUC)**: This measures the\n        two-dimensional area under the ROC Curve.  AUC is\n        scale-invariant and classification-threshold invariant.\n\n    - **ROC TP vs FP points**: In addition to a specific AUC score,\n        the performance at points\n\n    - **Confusion Matrix**: A confusion matrix is a table layout that\n        allows the visualization of the performance of an\n        algorithm. Each row of the matrix represents the instances in\n        an actual class while each column represents the instances in\n        a predicted class, or vice versa. It is a special kind of\n        contingency table, with two dimensions (\"actual\" and\n        \"predicted\"), and identical sets of \"classes\" in both\n        dimensions (each combination of dimension and class is a\n        variable in the contingency table.)\n\n  - **Prediction Bias**: The difference between the average of the\n      predicted labels and the average of the labels in the data\n      set.  One should check for prediction bias when evaluating the\n      classifier's results. Causes of bias can include:\n\n    - **Noisy data set**: Errors in original data can as the\n      collection method may have an underlying bias.\n\n    - **Processing bug**: Errors in the data pipeline can\n      introduce bias.\n\n    - **Biased training sample (unbalanced samples)**: Model\n      parameters may be skewed towards majority classes.\n\n    - **Overly strong regularization**: Model may be underfitting\n       model and too simple.\n\n    - **Proxy variables**: Model features may be highly\n       correlated.\n\n- **Supervised Learning**:\n\n  - **Overfitting and Underfitting**: Overfitting occurs when the the\n    model built corresponds too closely or exactly to a particular\n    set of data, and thus may fail to fit to predict additional data\n    reliably. An overfitted model is a mathematical model that\n    contains more parameters than can be justified by the data.\n    Underfitting occurs when the model built does adequately capture\n    the patterns in the data. As an example, a linear model will\n    underfit a non-linear dataset.\n\n  - **Sensitivity**: Perform N-fold Cross validation to indicate how\n    much sensitivity the algorithm has to data variation and to avoid\n    overfitting operational models.\n\n- **Decision Tree Learning**:\n\n  - **Sensitive to unbalanced classes**: Examine and determine target\n      class balance; decision tree learning algorithms are especially\n      sensitive to unbalanced target classes.\n\n  - **Consider decision boundaries**: Perform exploratory data\n      analysis to determine if decision boundaries lie alongaxes of\n      features. _Decision trees are ideal when decision boundaries can\n      be found that lie along axes of features._\n\n   - **Decision tree overfitting** may require tuning algorithm hyperparameters such as tree depth, max features used, max leaf nodes, etc.\n\n   - **Pruning** may result in a more robust model in real-word applications.\n\n   - **Missing values**: Inspect the data set to determine if there\n     are missing values and select a means to address them, either by\n     choosing an algorithm that works well or a way to impute the\n     value or eliminate the missing values in the data sensors or\n     pipeline.\n\n## Platforms, Tools, or Libraries\n\n- **scikit-learn**: includes tree algorithms for ID3, C4.5, C5.0, and CART.\n\n- **Weka**: includes J48 (C4.5), SimpleCart (CART), Logistic Model Trees, Naive Bayes Trees, and more.\n\n### Validation Approach\n- Use operationally relevant data across the range of application's operating environment.\n- Incorporate some kind of continuous validation to address concept drift and the need to retrain the model and/or check data quality.\n\n## References\n1. Decision tree learning. (2023, May 30). In _Wikipedia_. [Link](https://en.wikipedia.org/wiki/Decision_tree_learning).\n2. Decision Trees. (n.d.). In _scikit-learn User Guide 1.2.2_. [Link](https://scikit-learn.org/stable/modules/tree.html).\n3. Concept drift. (2023, April 17). In _Wikipedia_. [Link](https://en.wikipedia.org/wiki/Concept_drift).\n4. 8 Concept Drift Detection Methods. (n.d.). In _Aporia Learning Center_. [Link](https://www.aporia.com/learn/data-drift/concept-drift-detection-methods/).","rdfs:label":"Decision Tree","rdfs:subClassOf":{"@id":"d3f:Classification"}}]},"digital_artifacts":{"head":{"vars":["query_def_tech_label","top_def_tech_label","def_tech_label","def_artifact_rel_label","def_artifact_label","def_tactic","def_tactic_rel","def_tech","def_artifact_rel","def_artifact"]},"results":{"bindings":[]}},"subtechniques":{"@context":{"rdfs":"http://www.w3.org/2000/01/rdf-schema#","owl":"http://www.w3.org/2002/07/owl#","d3f":"http://d3fend.mitre.org/ontologies/d3fend.owl#","skos":"http://www.w3.org/2004/02/skos/core#"},"@graph":[]},"related_offensive_matrix":{},"references":{"@context":{"rdfs":"http://www.w3.org/2000/01/rdf-schema#","d3f":"http://d3fend.mitre.org/ontologies/d3fend.owl#"},"@graph":[]},"references_meta":{}}