{"def_to_off":{"head":{"vars":["query_def_tech_label","top_def_tech_label","def_tactic_label","def_tactic_rel_label","def_tech_label","def_artifact_rel_label","def_artifact_label","off_artifact_label","off_artifact_rel_label","off_tech_label","off_tech_id","off_tech_parent_label","off_tech_parent_is_toplevel","off_tactic_rel_label","off_tactic_label","def_tactic","def_tactic_rel","tactic_def_tech","def_tech","def_artifact_rel","def_artifact","off_artifact","off_artifact_rel","off_tech","off_tech_parent","off_tactic_rel","off_tactic"]},"results":{"bindings":[]}},"description":{"@context":{"rdfs":"http://www.w3.org/2000/01/rdf-schema#","owl":"http://www.w3.org/2002/07/owl#","d3f":"http://d3fend.mitre.org/ontologies/d3fend.owl#","skos":"http://www.w3.org/2004/02/skos/core#"},"@graph":[{"@id":"d3f:Classification","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":"D3A-CLA","d3f:definition":"Classification uses an algorithm to accurately assign test data into specific categories.","d3f:kb-article":"## How it works\nClassification recognizes specific entities within the dataset and attempts to draw some conclusions on how those entities should be labeled or defined. Common classification algorithms are linear classifiers, support vector machines (SVM), decision trees, k-nearest neighbor, and random forest, which are described in more detail below.\n\n## Considerations:\n\nThere are many different types of classification algorithms for modeling classification predictive modeling problems.\n\nThere is no single theory on how to map algorithms onto problem types; instead, it is generally recommended that a practitioner use controlled experiments and discover which algorithm and algorithm configuration results in the best performance for a given classification task.\n\n## Key Test Considerations\n\n- **Machine Learning**:\n\n  - **Verify the dataset quality**: Check the data to make sure it is\n      free of errors.  Quantify the degree of missing values,\n      outliers, and noise in the data collection.  If the data quality\n      is low, it may be difficult or impossible to create models and\n      systems with the desired performance.\n\n  - **Verify development datasets are representative** of expected\n      operational environment and data collection means.  Compare\n      distributions of dataset features and labels with exploratory\n      data analysis and assess the difference in tests on training\n      data and tests on evaluation data (where the evaluation data\n      must be drawn from a representative dataset.)\n\n  - **Use software libraries**: and tools built for ML where possible, so\n      that the underlying code is verified by prior use.**\n\n  - **Diagnose model errors with domain SMEs**: Have problem domain\n    SMEs investigate model errors for conditions for which the model\n    may underperform and suggest refinements.\n\n- **Classification**:\n\n  - **Use Standard Classification Performance Measures**: Not all of\n      the following may be necessary, but should be considered for\n      both verification (developmental test) and operational test\n      stages use:\n\n    - **Accuracy**: The fraction of predictions that were corret.\n\n    - **Precision**: The proportion of positive identifications that were correct.\n\n    - **Recall**: The proportion of actual positive cases identified correctly.\n\n    - **F-Measure**: Combines the preicion and recall into a single\n        score.  It is the harmonic mean of the precision and recall.\n\n    - **Receiver Operating Characteristic (ROC) Curve**: A ROC curve\n        shows the performance of a classification model at all\n        classification thresholds.  It graphs the True Positive Rate\n        over the False Positive Rate.\n\n    - **Area Under the ROC Curve (AUC)**: This measures the\n        two-dimensional area under the ROC Curve.  AUC is\n        scale-invariant and classification-threshold invariant.\n\n    - **ROC TP vs FP points**: In addition to a specific AUC score,\n        the performance at points\n\n    - **Confusion Matrix**: A confusion matrix is a table layout that\n        allows the visualization of the performance of an\n        algorithm. Each row of the matrix represents the instances in\n        an actual class while each column represents the instances in\n        a predicted class, or vice versa. It is a special kind of\n        contingency table, with two dimensions (\"actual\" and\n        \"predicted\"), and identical sets of \"classes\" in both\n        dimensions (each combination of dimension and class is a\n        variable in the contingency table.)\n\n  - **Prediction Bias**: The difference between the average of the\n      predicted labels and the average of the labels in the data\n      set.  One should check for prediction bias when evaluating the\n      classifier's results. Causes of bias can include:\n\n    - **Noisy data set**: Errors in original data can as the\n      collection method may have an underlying bias.\n\n    - **Processing bug**: Errors in the data pipeline can\n      introduce bias.\n\n    - **Biased training sample (unbalanced samples)**: Model\n      parameters may be skewed towards majority classes.\n\n\t- **Overly strong regularization**: Model may be underfitting\n        model and too simple.\n\n\t- **Proxy variables**: Model features may be highly\n        correlated.\n\n  - **Overfitting and Underfitting**: Overfitting occurs when the the\n    model built corresponds too closely or exactly to a particular\n    set of data, and thus may fail to fit to predict additional data\n    reliably. An overfitted model is a mathematical model that\n    contains more parameters than can be justified by the data.\n    Underfitting occurs when the model built does adequately capture\n    the patterns in the data. As an example, a linear model will\n    underfit a non-linear dataset.\n\n## Platforms, Tools, or Libraries:\n\n- **Python**:\n\n  - **scikit-learn**: Is a free software machine learning library for\n      Python and includes features for classification.\n\n  - **TensorFlow**: is an end-to-end source machine learning\n    platform.\n\n  - **Keras**: is an open-source library that provides a Python API\n    designed to enable fast experimentation with deep neural networks.\n\n  - **PyTorch**: Is a machine learning framework based on the Torch\n    library.\n\n- **R**:\n\n  - **caret**: Classification And REgression Training package contains\n      functions to streamline model training for complex regression\n      and classification problems.\n\n  - **randomForest**: Implementation of classification and regression\n      based on forest of trees.\n\n## References\n\n1. Supervised Learning. IBM.\n[Link](https://www.ibm.com/topics/supervised-learning).\n\n1. Types of Classification in Machine Learning. Machine Learning Mastery.\n[Link](https://machinelearningmastery.com/types-of-classification-in-machine-learning/).\n\n1. Google. (18 July 2022). Classification: Precision and Recall.\n[Link](https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall).\n\n1. Wikipedia. (18 Aug 2023). Overfitting.\n[Link](https://en.wikipedia.org/wiki/Overfitting).\n\n1. Wikipedia. (19 Aug 2023). Confusion matrix.\n[Link](https://en.wikipedia.org/wiki/Confusion_matrix).","rdfs:label":"Classification","rdfs:subClassOf":{"@id":"d3f:SupervisedLearning"}}]},"digital_artifacts":{"head":{"vars":["query_def_tech_label","top_def_tech_label","def_tech_label","def_artifact_rel_label","def_artifact_label","def_tactic","def_tactic_rel","def_tech","def_artifact_rel","def_artifact"]},"results":{"bindings":[]}},"subtechniques":{"@context":{"rdfs":"http://www.w3.org/2000/01/rdf-schema#","owl":"http://www.w3.org/2002/07/owl#","d3f":"http://d3fend.mitre.org/ontologies/d3fend.owl#","skos":"http://www.w3.org/2004/02/skos/core#"},"@graph":[]},"related_offensive_matrix":{},"references":{"@context":{"rdfs":"http://www.w3.org/2000/01/rdf-schema#","d3f":"http://d3fend.mitre.org/ontologies/d3fend.owl#"},"@graph":[]},"references_meta":{}}