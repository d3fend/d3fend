{"def_to_off":{"head":{"vars":["query_def_tech_label","top_def_tech_label","def_tactic_label","def_tactic_rel_label","def_tech_label","def_artifact_rel_label","def_artifact_label","off_artifact_label","off_artifact_rel_label","off_tech_label","off_tech_id","off_tech_parent_label","off_tech_parent_is_toplevel","off_tactic_rel_label","off_tactic_label","def_tactic","def_tactic_rel","tactic_def_tech","def_tech","def_artifact_rel","def_artifact","off_artifact","off_artifact_rel","off_tech","off_tech_parent","off_tactic_rel","off_tactic"]},"results":{"bindings":[]}},"description":{"@context":{"rdfs":"http://www.w3.org/2000/01/rdf-schema#","owl":"http://www.w3.org/2002/07/owl#","d3f":"http://d3fend.mitre.org/ontologies/d3fend.owl#","skos":"http://www.w3.org/2004/02/skos/core#"},"@graph":[{"@id":"d3f:AgglomerativeClustering","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":"D3A-AC","d3f:definition":"Agglomerative Clustering is a type of hierarchical clustering method where data points are grouped together based on similarity. Initially, each data point is treated as an individual cluster, and then in successive iterations, the closest clusters are merged until only one large cluster remains or until a specified stopping criterion is met.","d3f:kb-article":"## How it works\n\nAgglomerative clustering starts with each data point as its own cluster. The algorithm then iterates, identifying the two clusters that are closest to each other based on a defined distance metric (e.g., Euclidean, Manhattan). These two clusters are then merged into a single cluster. This process continues iteratively, merging the closest pairs of clusters in each step until all data points are merged into a single cluster or until other stopping criteria are achieved. A dendrogram, which is a tree-like diagram, can be used to represent the sequence of merges, providing a visual representation of the hierarchical structure of data.\n\n## Considerations\n\n- **Choice of Distance Metric**: The outcome can vary significantly depending on the chosen distance metric (e.g., Euclidean, Manhattan).\n\n- **Scalability**: Agglomerative clustering can be computationally intensive for large datasets.\n\n- **Sensitivity**: The method can be sensitive to outliers, which might affect the quality of the clusters formed.\n\n## Key Test Considerations\n\n- **Unsupervised Learning**:\n\n  - **Number of Clusters**: Determine an optimal number of clusters using the dendrogram and techniques like the elbow method.\n\n- **Cluster Analysis**:\n\n    - **Silhouette Score**: Evaluates how similar an object is to its own cluster compared to other clusters. A higher silhouette score indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters.\n\n    - **Dunn Index**: Measures the ratio between the smallest distance between observations not in the same cluster to the largest intra-cluster distance.\n\n- **Hierarchical Clustering**:\n\n    - **Cophenetic Correlation Coefficient**: Measures the correlation between the distances of points in feature space and their distances on the dendrogram. Helps assess the fidelity of the dendrogram in preserving pairwise distances between samples.\n\n- **Agglomerative Clustering**:\n\n    - **Linkage Criteria**: Test different linkage criteria (e.g., single, complete, average) to determine which produces the most cohesive clusters for the data at hand.\n\n  ## Platforms, Tools, or Libraries\n\n- **scikit-learn**:\n\n    - A versatile machine learning library in Python.\n\n    - The `AgglomerativeClustering` class in scikit-learn provides this functionality.\n\n- **SciPy**:\n\n    - A Python library used for scientific and technical computing.\n\n    - The `scipy.cluster.hierarchy` module provides functions for hierarchical and\n    agglomerative clustering, including the `linkage` and `dendrogram` functions.\n\n- **R**:\n\n    - The `hclust` function in the stats package provides agglomerative clustering.\n\n    - The `agnes` function in the `cluster` package offers a more extensive implementation.\n\n- **MATLAB**:\n\n    - Offers the `linkage` function for hierarchical agglomerative clustering and `dendrogram` for visualization.\n\n- **Weka**:\n\n    - A collection of machine learning algorithms for data mining tasks.\n\n    - The `HierarchicalClusterer` class provides an implementation of agglomerative clustering.\n\n## References\n\n1. Jain, A. K., & Dubes, R. C. (1988). *Algorithms for clustering data*. Prentice-Hall, Inc.\n\n2. Murtagh, F., & Legendre, P. (2014). Ward’s hierarchical agglomerative clustering method: which algorithms implement Ward’s criterion?. *Journal of Classification*, 31(3), 274-295. [Link](https://link.springer.com/article/10.1007/s00357-014-9161-z).\n\n3. Scikit-learn. (30 Jun 2023). Scikit-learn Documentation: Agglomerative Clustering.\n[Link](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html).","rdfs:label":"Agglomerative Clustering","rdfs:subClassOf":{"@id":"d3f:HierarchicalClustering"}}]},"digital_artifacts":{"head":{"vars":["query_def_tech_label","top_def_tech_label","def_tech_label","def_artifact_rel_label","def_artifact_label","def_tactic","def_tactic_rel","def_tech","def_artifact_rel","def_artifact"]},"results":{"bindings":[]}},"subtechniques":{"@context":{"rdfs":"http://www.w3.org/2000/01/rdf-schema#","owl":"http://www.w3.org/2002/07/owl#","d3f":"http://d3fend.mitre.org/ontologies/d3fend.owl#","skos":"http://www.w3.org/2004/02/skos/core#"},"@graph":[]},"related_offensive_matrix":{},"references":{"@context":{"rdfs":"http://www.w3.org/2000/01/rdf-schema#","d3f":"http://d3fend.mitre.org/ontologies/d3fend.owl#"},"@graph":[]},"references_meta":{}}